[
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Troubleshooting\nSome errors occur on more often than others. If you are experiencing an error for which you find a simple hack that you want to share, please add it to this troubleshooting guide.\nAs soon as the description for the hack is getting too long, we are going to add either a proper documentation page about the topic or we just implement it as a feature.\n\nRedis\nOn a Linux machine, if REDIS does not accept a connection, you can do this:\nredis-server --port {your redis port} --daemonize yes\nThe --daenonize yes parameter will enable the machine to run REDIS always in the background. If you do not use that parameter, the REDIS instance will stop as soon as you close the terminal where you started it.\nIf you are getting an error saying:\n15350:M 10 Dec 2024 23:24:17.172 # Warning: Could not create server TCP listening socket *:6379: bind: Address already in use\n15350:M 10 Dec 2024 23:24:17.172 # Failed listening on port 6379 (tcp), aborting.\nThen there might be already running a REDIS instance on that port.\nYou can verify that by typing:\nredis-cli -p {the redis port you had the error with}\nIf it opens the REDIS CLI, then you have an instance already running.\nTo check whether there are already keys in the REDIS server - assuming you have opened the REDIS CLI - you can type:\nkeys *\nThis will give you all the keys that are stored in the REDIS instance. If you are on a shared machine, you should always double-check whether no one else is using the same REDIS instance. Otherwise, it could easily happen that you overwrite someone else’s memory and data will be lost.\nOn Windows REDIS is run on WSL. Kill the session and restart it and run REDIS (if you have not set it to run automatically). Sometimes you may get in the situation that WSL will not open at all, in this case you need to restart the service called “Hyper-V Host Compute Service”\n\n\nSPI\nYou can get an error saying that the default path was not correct or that you get denied permission. Likely there was a connection implemented before and now when trying to start a new one it hangs itself up and refuses communication, you can force this communication channel by running\nsudo chmod 666 /dev/ttyACM0\n\n\nConcurrent operations\nIf two users access the same cluster at the same time, this message could be displayed\nKeyError: 'The acquisition data retrieved from the hardware does not contain data for acquisition channel 0 (referred to by Qblox acquisition index 0).\\n hardware_retrieved_acquisitions={}'\nContact the other users to avoid conflicts.",
    "crumbs": [
      "Home",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html",
    "href": "developer-guide/new_node_creation.html",
    "title": "Creating a new node",
    "section": "",
    "text": "This tutorial is about how to create a new node class. The word node is an overloaded term, used in many contexts, so, what does a node mean here? If you put all steps to characterize a qubit in a chain, then it could be seen as a directed graph with the calibration steps as nodes. Take a look at the node overview to get an impression on how it looks like. In this guide, we will cover the topics:\n\nFile locations for nodes within the framework\nHow to implement a node\nRegister the node in the framework\n\nFollowing these steps, you can easily contribute to the automatic calibration with your own nodes in a couple of hours.\n\n\nThe base classes on which all nodes are built can be found in tergite_autocalibration/lib/base. There are base classes for all three components of a node, the node itself, the measurement and the analysis.\nNodes can be either QubitNode or CouplerNode, both inherit from a BaseNode class providing common interfaces and functionalities. They will have different quantities of interest depending on the type (qubit_qoi for QubitNode) A Node also need to implement a Schedule; there are two files in tergite_autocalibration/lib/nodes - ScheduleNode: A node with a simple sweep over the samplespace. The quantify schedule is only compiled once and the parameter values from the samplespace are the input for the schedule function of the measurement. - ExternalParameterNode: A node with a more complex measurement procedure. It allows to run multiple steps sequentially, where each step might require recompilation of the schedule. There is an external parameter involved (the current applied to the coupler using the SPIRack), which is the part of the generating function within the schedule. There can be a fixed schedule or a schedule varying at each iteration (for example when changing value in the schedule when changing current). The latter can also handle elements (couplers or qubits) having different number of iterations.\nBoth files provide classes with multiple inheritance that combine type and schedule, i.e. ScheduleQubitNode or ExternalParameterFixedScheduleCouplerNode, that can be used in nodes to simplify implementation of new nodes for developers.\nMeasurements only have a base class providing a common interface.\nAnalysis classes have a more complex composition and inheritance structure. The entry point is always a BaseNodeAnalysis class, implemented in the BaseAllCouplersAnalysis or BaseAllQubitsAnalysis classes. Each node analysis will loop over all elements (qubits or couplers) defining a relevant analysis class, either QubitAnalysis or CouplerAnalysis. These analyses will have a common structure with a setup phase (where all dataset information is read and processed), a property that returns a quantity of interest (QOI) object, and then a call to a function updating the REDIS database. The analysis will be very node specific. CouplerAnalysis classes combine information from two QubitAnalyses; information can be stored both at the coupler level, for example all CZ gate information, or at the qubit level within the coupler (in REDIS couplers:coupler_name:qubit_name:qubit_information). This allows to handle information for the same qubit when used in different couplers. Plots are also handled in the analysis class, with the Qubit/Coupler analyses handling plotting according to the information required (i.e. plotting all qubits in a resonator_spectropy, both qubits in all couplers in a coupler_spectroscopy, or a single plot per coupler for two qubit gate randomized benchmarking).\n\n\n\nThe nodes are located in tergite_autocalibration/lib/nodes. If you open that module, you will find that there are four submodules for different kind of nodes:\n\ncharacterization: For nodes such as the T1, T2 or the randomized benchmarking.\ncoupler: For all nodes that are related to a two-qubit setup with a coupler.\nqubit_control: All nodes that calibrate a quantity of interest for the qubit.\nreadout: All nodes that calibrate a quantity of interest for the resonator.\n\nPlease create a new submodule for your node in one of the four submodules listed above. Essentially, a proper package in the end should contain:\n\n__init__.py: This is an empty file to mark that the folder is a package. Please add this file, because otherwise your classes cannot be found.\nnode.py: A file where the definition of the node class goes.\nanalysis.py: A file where the analysis object is defined.\nmeasurement.py: Contains the measurement object with the schedule.\ntests: A folder with all test function and test data. Read more about unit tests to find out on how to structure them.\nutils: A folder in case you have defined very specific helper classes.\n\nBefore we are going to a look on how this would be implemented in detail, a quick note on naming conventions.\n\n\nSince we are creating a lot of node, measurement and analysis objects, there are some naming conventions to make it more standardized and understandable to learn the framework. Taking the rabi oscillations as an example, we have:\n\nrabi_oscillations: This is the node name, which is used in the commandline interface or for other purposes to pass the node name as a string.\nRabiOscillations: This is the name of the node class.\nRabiOscillationsNodeAnalysis: The name of the respective analysis class for that node.\nRabiOscillationsMeasurement: And the name of the respective measurement class.\n\nWhen there are some more complicated nodes for which you do not know how to name it, please just take a look at the already existing nodes and make a guess how it feels to name it correctly. Also, when there is a node that starts with an abbreviation, please have all letter for the abbreviation capitalized e.g.: cz_calibration would be the node name for the CZCalibration node class.\n\n\n\n\nAll node classes are supposed follow the same interface as described in the BaseNodeclass. Below, for example, you have the rabi oscillations node:\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.schedule_node import ScheduleNode\nfrom tergite_autocalibration.lib.nodes.qubit_control.rabi_oscillations.measurement import RabiOscillationsMeasurement\nfrom tergite_autocalibration.lib.nodes.qubit_control.rabi_oscillations.analysis import RabiQubitAnalysis\n\nclass RabiOscillationsNode(ScheduleQubitNode):\n  measurement_obj = RabiOscillationsMeasurement\n  analysis_obj = RabiQubitAnalysis\n  qubit_qois = [\"rxy:amp180\"]\n\n  def __init__(self, name: str, all_qubits: list[str], **schedule_keywords):\n    super().__init__(name, all_qubits, **schedule_keywords)\n    self.schedule_samplespace = {\n      \"mw_amplitudes\": {\n        qubit: np.linspace(0.002, 0.90, 61) for qubit in self.all_qubits\n      }\n    }\nAs you can see, it inherits from the ScheduleQubitNode class, which contains a very simple definition of a node that runs a simple sweep over a single quantity for all qubits defined in the run_config. More information about other node classes can be found in the description of the base classes above. Furthermore, you can see that the node has three class attributes:\n\nmeasurement_obj: Contains the class of the measurement, that defines the pulse schedule for the instruments\nanalysis_obj: Contains the class for the analysis to post-process the measurement results. For example, this could be a simple fitting function.\n\nAlso, you can see in the constructor, there is an attribute called schedule_samplespace. Here, we define in the measurement, what quantity will be swept over.\n\n\nThe measurement_obj is implemented in the measurement.py file of your node submodule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node’s samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is implemented in the analysis.py file from your module and contains the class that perform the analysis. See the description in the Base classes above to understand which classes need to be implemented and which base classes to use.\nSome useful information: - Node analyses will simply loop over Qubit/Coupler analyses and will automatically deal with high level events such as plotting and saving to REDIS. Operations can be overloaded if a node has specific needs. The most common custom operation at this level is the definition of which additional plots are plotted. - Coupler/Qubit analyses have two steps, setup (dataset handling) and analysis. - Analyses return an object called Quantity Of Interest (QOI) which is a dictionary with a success boolean variable and a dictionary called analysis result that can stored any type of information. Normally the QOI have the same names of the redis fields but this is not mandatory. For example in the cz_calibration_analysis the QOIs are different from the redis_fields defined in the dynamic calibration nodes. - In addition to the standard plots that are displayed, there is another function called save_other_plots that can be implemented in each analysis to plot additional figures. This can be extremely useful for debugging and for visualizing additional information, for example in coupler nodes dealing with multiple working points. - Information can be stored in REDIS for each qubit in a coupler. This is done using this structure in the analysis result of the QOI:\n        analysis_result = {\n            self.name_qubit_1: (dict(zip(self.redis_fields, q1result))),\n            self.name_qubit_2: (dict(zip(self.redis_fields, q2result))),\n        }\nAn example can be found in the coupler spectroscopy and in the coupler resonator spectroscopy coupler analyses.\n\n\n\n\nTo add the node to the framework, you have to register it in two places - the node factory and the calibration graph. Also, please do not forget to write documentation for your node.\n\n\nA factory is a programming pattern to create complex objects such as our nodes in a bit more clearly interfaced way. The factory contains a register for all nodes, that map their name to the respective class where the node is implemented. When you are adding a node, please register your node name, by adding it to the tergite_autocalibration.lib.utils.node_factory.NodeFactory class under the self.node_name_mapping attribute in the dictionary.\n\n\n\nIn the file tergite_autocalibration/lib/utils/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\nIt is possible to have multiple dependencies, i.e. a node can be set to run only if two different nodes are run.\n\n('previous_node_1','new_node')\n('previous_node_2','new_node')\n\nThis will result in both previous_node_1 and previous_node_2 to be run before the new_node.\n\n\n\nPlease add your node to the list of available nodes in this documentation. If possible, please create a separate page that explains what your node is doing and link it there.\nAdd any relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.\n\n\n\nFigure are managed from the figure_util.py file in the util folder of the base node directory. All figure should be created by provided utility function to assure consistent formatting. It is possible to create additional figure by implementing the “save_other_plots” function in your analysis node. Please create the new figure using this utility function. The typical usage is to display all 1D distributions when performing a 2D scan. An example can be found in the punchout node.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#base-classes",
    "href": "developer-guide/new_node_creation.html#base-classes",
    "title": "Creating a new node",
    "section": "",
    "text": "The base classes on which all nodes are built can be found in tergite_autocalibration/lib/base. There are base classes for all three components of a node, the node itself, the measurement and the analysis.\nNodes can be either QubitNode or CouplerNode, both inherit from a BaseNode class providing common interfaces and functionalities. They will have different quantities of interest depending on the type (qubit_qoi for QubitNode) A Node also need to implement a Schedule; there are two files in tergite_autocalibration/lib/nodes - ScheduleNode: A node with a simple sweep over the samplespace. The quantify schedule is only compiled once and the parameter values from the samplespace are the input for the schedule function of the measurement. - ExternalParameterNode: A node with a more complex measurement procedure. It allows to run multiple steps sequentially, where each step might require recompilation of the schedule. There is an external parameter involved (the current applied to the coupler using the SPIRack), which is the part of the generating function within the schedule. There can be a fixed schedule or a schedule varying at each iteration (for example when changing value in the schedule when changing current). The latter can also handle elements (couplers or qubits) having different number of iterations.\nBoth files provide classes with multiple inheritance that combine type and schedule, i.e. ScheduleQubitNode or ExternalParameterFixedScheduleCouplerNode, that can be used in nodes to simplify implementation of new nodes for developers.\nMeasurements only have a base class providing a common interface.\nAnalysis classes have a more complex composition and inheritance structure. The entry point is always a BaseNodeAnalysis class, implemented in the BaseAllCouplersAnalysis or BaseAllQubitsAnalysis classes. Each node analysis will loop over all elements (qubits or couplers) defining a relevant analysis class, either QubitAnalysis or CouplerAnalysis. These analyses will have a common structure with a setup phase (where all dataset information is read and processed), a property that returns a quantity of interest (QOI) object, and then a call to a function updating the REDIS database. The analysis will be very node specific. CouplerAnalysis classes combine information from two QubitAnalyses; information can be stored both at the coupler level, for example all CZ gate information, or at the qubit level within the coupler (in REDIS couplers:coupler_name:qubit_name:qubit_information). This allows to handle information for the same qubit when used in different couplers. Plots are also handled in the analysis class, with the Qubit/Coupler analyses handling plotting according to the information required (i.e. plotting all qubits in a resonator_spectropy, both qubits in all couplers in a coupler_spectroscopy, or a single plot per coupler for two qubit gate randomized benchmarking).",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#where-are-the-nodes-located",
    "href": "developer-guide/new_node_creation.html#where-are-the-nodes-located",
    "title": "Creating a new node",
    "section": "",
    "text": "The nodes are located in tergite_autocalibration/lib/nodes. If you open that module, you will find that there are four submodules for different kind of nodes:\n\ncharacterization: For nodes such as the T1, T2 or the randomized benchmarking.\ncoupler: For all nodes that are related to a two-qubit setup with a coupler.\nqubit_control: All nodes that calibrate a quantity of interest for the qubit.\nreadout: All nodes that calibrate a quantity of interest for the resonator.\n\nPlease create a new submodule for your node in one of the four submodules listed above. Essentially, a proper package in the end should contain:\n\n__init__.py: This is an empty file to mark that the folder is a package. Please add this file, because otherwise your classes cannot be found.\nnode.py: A file where the definition of the node class goes.\nanalysis.py: A file where the analysis object is defined.\nmeasurement.py: Contains the measurement object with the schedule.\ntests: A folder with all test function and test data. Read more about unit tests to find out on how to structure them.\nutils: A folder in case you have defined very specific helper classes.\n\nBefore we are going to a look on how this would be implemented in detail, a quick note on naming conventions.\n\n\nSince we are creating a lot of node, measurement and analysis objects, there are some naming conventions to make it more standardized and understandable to learn the framework. Taking the rabi oscillations as an example, we have:\n\nrabi_oscillations: This is the node name, which is used in the commandline interface or for other purposes to pass the node name as a string.\nRabiOscillations: This is the name of the node class.\nRabiOscillationsNodeAnalysis: The name of the respective analysis class for that node.\nRabiOscillationsMeasurement: And the name of the respective measurement class.\n\nWhen there are some more complicated nodes for which you do not know how to name it, please just take a look at the already existing nodes and make a guess how it feels to name it correctly. Also, when there is a node that starts with an abbreviation, please have all letter for the abbreviation capitalized e.g.: cz_calibration would be the node name for the CZCalibration node class.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#node-implementation-details",
    "href": "developer-guide/new_node_creation.html#node-implementation-details",
    "title": "Creating a new node",
    "section": "",
    "text": "All node classes are supposed follow the same interface as described in the BaseNodeclass. Below, for example, you have the rabi oscillations node:\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.schedule_node import ScheduleNode\nfrom tergite_autocalibration.lib.nodes.qubit_control.rabi_oscillations.measurement import RabiOscillationsMeasurement\nfrom tergite_autocalibration.lib.nodes.qubit_control.rabi_oscillations.analysis import RabiQubitAnalysis\n\nclass RabiOscillationsNode(ScheduleQubitNode):\n  measurement_obj = RabiOscillationsMeasurement\n  analysis_obj = RabiQubitAnalysis\n  qubit_qois = [\"rxy:amp180\"]\n\n  def __init__(self, name: str, all_qubits: list[str], **schedule_keywords):\n    super().__init__(name, all_qubits, **schedule_keywords)\n    self.schedule_samplespace = {\n      \"mw_amplitudes\": {\n        qubit: np.linspace(0.002, 0.90, 61) for qubit in self.all_qubits\n      }\n    }\nAs you can see, it inherits from the ScheduleQubitNode class, which contains a very simple definition of a node that runs a simple sweep over a single quantity for all qubits defined in the run_config. More information about other node classes can be found in the description of the base classes above. Furthermore, you can see that the node has three class attributes:\n\nmeasurement_obj: Contains the class of the measurement, that defines the pulse schedule for the instruments\nanalysis_obj: Contains the class for the analysis to post-process the measurement results. For example, this could be a simple fitting function.\n\nAlso, you can see in the constructor, there is an attribute called schedule_samplespace. Here, we define in the measurement, what quantity will be swept over.\n\n\nThe measurement_obj is implemented in the measurement.py file of your node submodule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node’s samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is implemented in the analysis.py file from your module and contains the class that perform the analysis. See the description in the Base classes above to understand which classes need to be implemented and which base classes to use.\nSome useful information: - Node analyses will simply loop over Qubit/Coupler analyses and will automatically deal with high level events such as plotting and saving to REDIS. Operations can be overloaded if a node has specific needs. The most common custom operation at this level is the definition of which additional plots are plotted. - Coupler/Qubit analyses have two steps, setup (dataset handling) and analysis. - Analyses return an object called Quantity Of Interest (QOI) which is a dictionary with a success boolean variable and a dictionary called analysis result that can stored any type of information. Normally the QOI have the same names of the redis fields but this is not mandatory. For example in the cz_calibration_analysis the QOIs are different from the redis_fields defined in the dynamic calibration nodes. - In addition to the standard plots that are displayed, there is another function called save_other_plots that can be implemented in each analysis to plot additional figures. This can be extremely useful for debugging and for visualizing additional information, for example in coupler nodes dealing with multiple working points. - Information can be stored in REDIS for each qubit in a coupler. This is done using this structure in the analysis result of the QOI:\n        analysis_result = {\n            self.name_qubit_1: (dict(zip(self.redis_fields, q1result))),\n            self.name_qubit_2: (dict(zip(self.redis_fields, q2result))),\n        }\nAn example can be found in the coupler spectroscopy and in the coupler resonator spectroscopy coupler analyses.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#register-the-node-in-the-framework",
    "href": "developer-guide/new_node_creation.html#register-the-node-in-the-framework",
    "title": "Creating a new node",
    "section": "",
    "text": "To add the node to the framework, you have to register it in two places - the node factory and the calibration graph. Also, please do not forget to write documentation for your node.\n\n\nA factory is a programming pattern to create complex objects such as our nodes in a bit more clearly interfaced way. The factory contains a register for all nodes, that map their name to the respective class where the node is implemented. When you are adding a node, please register your node name, by adding it to the tergite_autocalibration.lib.utils.node_factory.NodeFactory class under the self.node_name_mapping attribute in the dictionary.\n\n\n\nIn the file tergite_autocalibration/lib/utils/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\nIt is possible to have multiple dependencies, i.e. a node can be set to run only if two different nodes are run.\n\n('previous_node_1','new_node')\n('previous_node_2','new_node')\n\nThis will result in both previous_node_1 and previous_node_2 to be run before the new_node.\n\n\n\nPlease add your node to the list of available nodes in this documentation. If possible, please create a separate page that explains what your node is doing and link it there.\nAdd any relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.\n\n\n\nFigure are managed from the figure_util.py file in the util folder of the base node directory. All figure should be created by provided utility function to assure consistent formatting. It is possible to create additional figure by implementing the “save_other_plots” function in your analysis node. Please create the new figure using this utility function. The typical usage is to display all 1D distributions when performing a 2D scan. An example can be found in the punchout node.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/node_classes.html",
    "href": "developer-guide/node_classes.html",
    "title": "Node classes",
    "section": "",
    "text": "Node classes\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibility in the node implementations the nodes are categorized:\n\nScheduleNode: The simple way of doing the measurement and having an analysis afterward. This node compiles one time\n\nThere is only node.schedule_samplespace if the sweeping takes place within the schedule.\n\nExternalParameterNode: A looping over an external parameter during the sweep. This node compiles several times.\n\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside the schedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\nBelow, there is an example for an ExternalParameterNode implementation.\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.coupler.spectroscopy.analysis import (\n    CouplerSpectroscopyNodeAnalysis,\n)\n\nfrom tergite_autocalibration.lib.nodes.external_parameter_node import (\n    ExternalParameterNode,\n)\n\nfrom tergite_autocalibration.lib.nodes.qubit_control.spectroscopy.measurement import (\n    TwoTonesMultidimMeasurement,\n)\n\nfrom tergite_autocalibration.lib.utils.samplespace import qubit_samples\nfrom tergite_autocalibration.utils.dto.enums import MeasurementMode\nfrom tergite_autocalibration.utils.hardware.spi import SpiDAC\n\n\nclass CouplerSpectroscopyNode(ExternalParameterNode):\n    measurement_obj = TwoTonesMultidimMeasurement\n    analysis_obj = CouplerSpectroscopyNodeAnalysis\n    coupler_qois = [\"parking_current\", \"current_range\"]\n\n    def __init__(\n            self, name: str, all_qubits: list[str], couplers: list[str], **schedule_keywords\n    ):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.name = name\n        self.couplers = couplers\n        self.qubit_state = 0\n        self.schedule_keywords[\"qubit_state\"] = self.qubit_state\n        self.coupled_qubits = self.get_coupled_qubits()\n        self.coupler = self.couplers[0]\n\n        self.mode = MeasurementMode.real\n        self.spi_dac = SpiDAC(self.mode)\n        self.dac = self.spi_dac.create_spi_dac(self.coupler)\n\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            \"spec_frequencies\": {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            \"dc_currents\": {self.coupler: np.arange(-2.5e-3, 2.5e-4, 280e-6)},\n        }\n\n    def get_coupled_qubits(self) -&gt; list:\n        if len(self.couplers) &gt; 1:\n            print(\"Multiple couplers, lets work with only one\")\n        coupled_qubits = self.couplers[0].split(sep=\"_\")\n        self.coupler = self.couplers[0]\n        return coupled_qubits\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space[\"dc_currents\"]\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f\"{ this_iteration_value = }\")\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\n\n    def final_operation(self):\n        print(\"Final Operation\")\n        self.spi_dac.set_dac_current(self.dac, 0)\nPlease read the guide about how to create a new node to learn more about nodes. This guide also contains an example for a ScheduleNode.\nExamples of nodes requiring an external samplespace\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetition index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Node Classes"
    ]
  },
  {
    "objectID": "developer-guide/debugging.html",
    "href": "developer-guide/debugging.html",
    "title": "Debugging",
    "section": "",
    "text": "Debugging in programming is the process of identifying, analyzing, and fixing errors (bugs) in a computer program. It involves running the code, detecting issues, and correcting them to ensure the program behaves as expected.\nKnowing how to debug can also make you understand a new code base faster and with more ease. In the following, there are a three debugging techniques, explained with advantages and disadvantages.\n\nPrint statements or loggers\nBreakpoints\nGraphical debugger\n\nThis guide should help you to get a first insight into debugging and can be a starting point before searching the internet for more information about advanced techniques.\n\n\nThe most easy way to find out what an application is doing, is by adding print statements or logging to the code. Let us say we are having the following code:\nif __name__ == '__main__':\n    x = 5\n    y = x + 1\n    print(x)\nAdding the print statement like this is the most intuitive way to find out more about the state of our application. Likewise, the print statement can also be replaced by a logger on a very low verbosity to catch all related debugging messages.\nfrom tergite_autocalibration.utils.logging import logger\n\nlogger.debug(\"A message to be only printed while debugging the application.\")\nConsider reading more about the logger before starting.\nAdvantages\n\nIt is very easy to add the print statement.\nYou can easily add print statements even when you are using console based editors.\n\nDisadvantages\n\nIf you add too many print statements, the console output can become very verbose\nYou have to know in advance what you want to print. Let us say, in the example above, you wanted to print y as well and it would take a long time to compute other things before you reach y in the code, you are loosing a long time to wait.\nYou have to define print statements also for more complex variables.\n\n\n\n\nTo overcome these issues, Python offers built-in support for so-called breakpoints. The idea behind a breakpoint is simple. The code will execute until it reaches the breakpoint. Then it will open an interactive console where you can use the variables defined in this very moment of the execution in an ipython manner. Like that, you can print values and explore variables a bit more flexibly.\nclass AdvancedObject:\n    \n    def __init__(self):\n        self.var_a = 0\n        self.var_b = 1\n\nif __name__ == '__main__':\n    x = 5\n    y = x + 1\n    advanced_object = AdvancedObject()\n    \n    breakpoint()\nThe extended example now uses the breakpoint instead of the print statement. When you run the code with the pydb command, it will open the ipython-like console when it comes to breakpoint.\npython pdb my_script.py\nYou can find more information about the python debugger in the official documentation for pdb and this tutorial.\nAdvantages\n\nLets you find out more about the state of the application in an interactive way.\nNot much more complicated than the print statements.\nPart of the Python standard library.\n\nDisadvantages\n\nYou still have to remove the breakpoints afterward to clean up your code.\nFor very complex objects, it is still complicated to discover their structure.\n\n\n\n\nModern IDEs nowadays usually feature a built-in debugger by default. The debugger often offers a way to evaluate expressions during runtime. Breakpoints can be set just for the IDE itself even during runtime, and they are usually not destructive i.e. part of the codebase.\n\n\n\nGraphical debugger in PyCharm\n\n\nOn the picture above, you can see how the graphical debugger looks like in PyCharm. There are five important areas:\n\nblue: This is inside the actual code. The red dot marks the position in the code, where it should halt during execution.\nred: On this panel, you can control the code execution. There are buttons to restart, stop, continue to the next breakpoint, halt, go to the next line and more. The choices might vary for each IDE.\norange: Since the function you are inside can possibly been called by another function outside, on the orange panel there is a stack trace where you can navigate between the contexts.\npink: This section shows the local and global variables active for this current context. You can easily explore objects and nested structures.\ngreen: In case you want to evaluate a variable or try some code, you can run it in the evaluation box. This will execute your code on top of the state your application is in at this very moment. This means you can use all defined variables and play around with them.\n\nLearning how to use such a debugger is super easy. It is a real game changer and will save you a lot of time while trying to figure out why your code is hanging somewhere. For more information check out these guides for VSCode and PyCharm.\nAdvantages\n\nYou can easily explore very complex states of your application.\nNo need to remove print statements or breakpoints.\nMost IDEs include a debugger out of the box.\n\nDisadvantages\n\nThe code execution might be a bit slower than usual.\nSometimes the debugger introduces errors, or you will end up in a strange state. Especially, when working with multithreaded applications. However, this does not happen very often and will probably not happen for the automatic calibration.\n\n\n\n\nThis article introduced some methods on how to debug applications and deal with errors. If you are interested in more techniques, consider reading about:\n\nRubber duck debugging\nCode reviews\nOr continue with the next article about writing documentation",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Debugging"
    ]
  },
  {
    "objectID": "developer-guide/debugging.html#next-steps",
    "href": "developer-guide/debugging.html#next-steps",
    "title": "Debugging",
    "section": "",
    "text": "This article introduced some methods on how to debug applications and deal with errors. If you are interested in more techniques, consider reading about:\n\nRubber duck debugging\nCode reviews\nOr continue with the next article about writing documentation",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Debugging"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Documentation of the Tergite Automatic Calibration",
    "section": "",
    "text": "Documentation of the Tergite Automatic Calibration\nThe tergite-autocalibration is a toolkit to ease calibrating quantum devices for superconducting platforms. The project contains an orchestration manager, a collection of calibration schedules and a collection of post-processing and analysis routines. It is tailored to tune-up the 25 qubits QPU at Chalmers, which is receiving generous funding by the Wallenberg Centre for Quantum Technology (WACQT) for research, development and operation.\n\nUser Guide\nA tutorial on how to get started with the automatic calibration. This tutorial contains a guide on how to use the commandline interface with quick commands. Further, there is an introduction into configuration files.\n\nGetting started\nOperation\nConfiguration Files\n\n\n\nNode Library\nThe main principle behind the automatic calibration is based on calibrating nodes in a graph structure. A node contains all the measurement and analysis classes to find the quantity of interest - for qubits and couplers. If you are interested in implementing a new node, it might be worth it to check whether there are existing nodes that you can use to find the qubit properties you are looking to calibrate.\n\n\nDeveloper Guide\nThis repository is an actively developed open-source project and also part of the Tergite full-stack software ecosystem to operate a quantum computer. Hence, you are more than welcome to contribute to the project with your own ideas that fit into the framework. To familiarize yourself with the existing classes and the architecture of the automatic calibration, please take a look into the development guide. Here, you can also find best practices and information about our design philosophy.\n\nCreate node classes\nWrite unit tests\nDocumentation guidelines",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "available_nodes.html",
    "href": "available_nodes.html",
    "title": "Node Library",
    "section": "",
    "text": "graph TD\n    A[Resonator Spectroscopy] --&gt; A1[Punchout]\n    A1 --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n    C --&gt; D[Ramsey Correction]\n    D --&gt; E[Motzoi Parameter]\n    E --&gt; F[Resonator Spectroscopy 1]\n    F --&gt; C1[T1] --&gt; C2[T2] --&gt; C3[Randomized Benchmarking]\n    F --&gt; F1(Qubit 12 Spectroscopy) --&gt; G(Rabi 12 Oscillations) --&gt; G1[Resonator Spectroscopy 2]\n    F --&gt; H1(2 States Discrimination)\n    G1 --&gt; H2(3 States Discrimination)\n    A --&gt; I(Resonator spectroscopy vs current)\n    B --&gt; J(Qubit spectroscopy vs current)\n    I --&gt; J\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n    click F href \"nodes/resonator_spectroscopy_node.html\"\n    click G1 href \"nodes/resonator_spectroscopy_node.html\"\n    click A1 href \"nodes/punchout_node.html\"\n    click B href \"nodes/qubit_spectroscopy_node.html\"\n    click F1 href \"nodes/qubit_spectroscopy_node.html\"\n    click C href \"nodes/rabi_oscillations_node.html\"\n    click I href \"nodes/resonator_spectroscopy_vs_current_node.html\"\n    click J href \"nodes/qubit_spectroscopy_vs_current_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style A1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style D fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style E fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style F fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C1 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C2 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C3 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style F1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style G fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style G1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H2 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style I fill:#ff9999,stroke:#333,stroke-width:2px\n    style J fill:#ff9999,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\nresonator_spectroscopy\npunchout\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization\n\n\n\n\n\nqubit_01_spectroscopy\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\n\n\n\n\n\ncoupler_spectroscopy\ncoupler_resonator_spectroscopy\n\n\n\n\n\nT1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#readout-nodes",
    "href": "available_nodes.html#readout-nodes",
    "title": "Node Library",
    "section": "",
    "text": "resonator_spectroscopy\npunchout\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#qubit-control-nodes",
    "href": "available_nodes.html#qubit-control-nodes",
    "title": "Node Library",
    "section": "",
    "text": "qubit_01_spectroscopy\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#coupler-nodes",
    "href": "available_nodes.html#coupler-nodes",
    "title": "Node Library",
    "section": "",
    "text": "coupler_spectroscopy\ncoupler_resonator_spectroscopy",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#characterization-nodes",
    "href": "available_nodes.html#characterization-nodes",
    "title": "Node Library",
    "section": "",
    "text": "T1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "nodes/rabi_oscillations_node.html",
    "href": "nodes/rabi_oscillations_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The rabi oscillations are essential when determining the amplitude of the signal send to the qubit. The (\\(\\pi\\))-pulses, which amplitudes is varying is a Gaussian pulses. By varying the amplitude a cosine function appears when measuring transmission coefficient (S21) and with that one can determine the optimal amplitude for qubit operations. This can also be done for 12 operations with the only difference being the 1 state being initialized.\n\n\nThe RabiOscillationsMeasurement class facilitates the creation of schedules for Rabi oscillations.\n\n\nThe schedule_function generates an experimental schedule for performing Rabi oscillations. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nAmplitude Sweeping: Iteratively adjusts the probing amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nmw_amplitudes (dict[str, np.ndarray]): Amplitudes of the probing (\\(\\pi\\))-pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe RabiQubitAnalysis class analyzes the results of Rabi oscillations experiments. It fits a cosine function to the qubit response for different amplitudes of the (\\(\\pi\\))-pulse, determinaning the Rabi frequencies.\n\n\nThe analyse_qubit method processes the Rabi oscillations data to extract key parameters:\n\nOptimal (\\(\\pi\\))-pulse amplitude: The optimal amplitude resulting in the qubit getting to the desired state.\n\nSteps:\n\nLoad Data:\nExtract the drive amplitude and magnitude values from the dataset for analysis.\nParameter Guessing:\n\nCompute an initial guess for the cosine model parameters:\n\nAmplitude: Based on the difference between maximum and minimum magnitudes.\nOffset: As the mean of the magnitude values.\nFrequency: Using a Fast Fourier Transform (FFT) to estimate oscillation frequency.\n\nUpdate the model parameters with the computed guesses.\n\nFit Model:\nFit the cosine function to the Rabi oscillation data using the guessed parameters. The model adjusts to minimize the error.\nExtract Results:\n\nCalculate the (\\(\\pi\\))-pulse amplitude (amp180) from the fit.\nEstimate the uncertainty of the (\\(\\pi\\))-pulse amplitude using the standard error from the fit results.\n\nGenerate Fit Curve:\nEvaluate the fitted model over a finer set of amplitude values for smooth plotting.\nPlot Results:\n\nPlot the fitted curve against the original data points.\nAnnotate the plot with the (\\(\\pi\\))-pulse amplitude value.",
    "crumbs": [
      "Home",
      "Node Library",
      "Rabi oscillations"
    ]
  },
  {
    "objectID": "nodes/rabi_oscillations_node.html#rabi-oscillations-calibration-and-analysis",
    "href": "nodes/rabi_oscillations_node.html#rabi-oscillations-calibration-and-analysis",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The rabi oscillations are essential when determining the amplitude of the signal send to the qubit. The (\\(\\pi\\))-pulses, which amplitudes is varying is a Gaussian pulses. By varying the amplitude a cosine function appears when measuring transmission coefficient (S21) and with that one can determine the optimal amplitude for qubit operations. This can also be done for 12 operations with the only difference being the 1 state being initialized.\n\n\nThe RabiOscillationsMeasurement class facilitates the creation of schedules for Rabi oscillations.\n\n\nThe schedule_function generates an experimental schedule for performing Rabi oscillations. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nAmplitude Sweeping: Iteratively adjusts the probing amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nmw_amplitudes (dict[str, np.ndarray]): Amplitudes of the probing (\\(\\pi\\))-pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe RabiQubitAnalysis class analyzes the results of Rabi oscillations experiments. It fits a cosine function to the qubit response for different amplitudes of the (\\(\\pi\\))-pulse, determinaning the Rabi frequencies.\n\n\nThe analyse_qubit method processes the Rabi oscillations data to extract key parameters:\n\nOptimal (\\(\\pi\\))-pulse amplitude: The optimal amplitude resulting in the qubit getting to the desired state.\n\nSteps:\n\nLoad Data:\nExtract the drive amplitude and magnitude values from the dataset for analysis.\nParameter Guessing:\n\nCompute an initial guess for the cosine model parameters:\n\nAmplitude: Based on the difference between maximum and minimum magnitudes.\nOffset: As the mean of the magnitude values.\nFrequency: Using a Fast Fourier Transform (FFT) to estimate oscillation frequency.\n\nUpdate the model parameters with the computed guesses.\n\nFit Model:\nFit the cosine function to the Rabi oscillation data using the guessed parameters. The model adjusts to minimize the error.\nExtract Results:\n\nCalculate the (\\(\\pi\\))-pulse amplitude (amp180) from the fit.\nEstimate the uncertainty of the (\\(\\pi\\))-pulse amplitude using the standard error from the fit results.\n\nGenerate Fit Curve:\nEvaluate the fitted model over a finer set of amplitude values for smooth plotting.\nPlot Results:\n\nPlot the fitted curve against the original data points.\nAnnotate the plot with the (\\(\\pi\\))-pulse amplitude value.",
    "crumbs": [
      "Home",
      "Node Library",
      "Rabi oscillations"
    ]
  },
  {
    "objectID": "user-guide/operation.html",
    "href": "user-guide/operation.html",
    "title": "Operation",
    "section": "",
    "text": "The package ships with a CLI called acli (autocalibration command line interface) to solve some common tasks that appear quite often. In the following there are a number of useful commands, but if you want to find out information about commands in your shell use acli --help.\nSince some of the commands below are using autocompleting in most cases you would have to enable that feature in your shell by running:\nacli --install-completion\nUsually the shell would just suggest you file paths when pressing the tabulator, but with autocompletion enabled, you would be even get suggestions for node names or other inputs depending on your configuration.\n\n\nThis section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\nconfig: Load and save the configuration files\ncalibration: Handle operations related to the calibration supervisor\nbrowser: Will open the dataset browser, which makes you view the datasets from measurements\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nLoad the configuration.\nUsage:\nacli config load [OPTIONS]\nOptions:\n\n-f/--filepath: Path to the configuration package to load. It can be either to the configuration.meta.toml file or to a zip file containing the whole configuration.\n-t/--template: Path to the template package to load. The templates are located in tergite_autocalibration.config.templates. If the autocompletion is installed for acli, then the templates should be shown as suggestions automatically.\n\nNotes:\nTo run this command, please navigate to the root directory of the repository. The configuration package will be placed into the root directory, which is the default location for the application to detect the configuration package.\n\n\n\nUsage:\nacli config save [OPTIONS]\nSave the configuration.\nOptions:\n\n-f/--filepath: Path to the configuration package to save. If the path name is ending with .zip, it will automatically create a zip file and treat it as if you are running with -z.\n-z/--as-zip: Will make the configuration file be a zip archive.\n\n\n\n\nUsage:\nacli quickstart [OPTIONS]\nLoads the default configuration package. This configuration package is not supposed to run on any setup, but it will just copy configuration files into the application, so, that the application does not crash because configuration files are missing.\nOptions:\n\n-q, --qubits TEXT: Indicates which qubits should be in the template e.g. \"q00,q01\" or \"q03-q05\", \"q01-q03, q07\" or an integer e.g. 3 to generate \"q01, q02, q03\".\n\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#cli",
    "href": "user-guide/operation.html#cli",
    "title": "Operation",
    "section": "",
    "text": "This section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\nconfig: Load and save the configuration files\ncalibration: Handle operations related to the calibration supervisor\nbrowser: Will open the dataset browser, which makes you view the datasets from measurements\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nLoad the configuration.\nUsage:\nacli config load [OPTIONS]\nOptions:\n\n-f/--filepath: Path to the configuration package to load. It can be either to the configuration.meta.toml file or to a zip file containing the whole configuration.\n-t/--template: Path to the template package to load. The templates are located in tergite_autocalibration.config.templates. If the autocompletion is installed for acli, then the templates should be shown as suggestions automatically.\n\nNotes:\nTo run this command, please navigate to the root directory of the repository. The configuration package will be placed into the root directory, which is the default location for the application to detect the configuration package.\n\n\n\nUsage:\nacli config save [OPTIONS]\nSave the configuration.\nOptions:\n\n-f/--filepath: Path to the configuration package to save. If the path name is ending with .zip, it will automatically create a zip file and treat it as if you are running with -z.\n-z/--as-zip: Will make the configuration file be a zip archive.\n\n\n\n\nUsage:\nacli quickstart [OPTIONS]\nLoads the default configuration package. This configuration package is not supposed to run on any setup, but it will just copy configuration files into the application, so, that the application does not crash because configuration files are missing.\nOptions:\n\n-q, --qubits TEXT: Indicates which qubits should be in the template e.g. \"q00,q01\" or \"q03-q05\", \"q01-q03, q07\" or an integer e.g. 3 to generate \"q01, q02, q03\".\n\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html",
    "href": "user-guide/configuration_files.html",
    "title": "Configuration",
    "section": "",
    "text": "To run the autocalibration you will need to configure it with a couple of different configuration files. Doing the configuration with a pre-built configuration package will take just some minutes. On an unknown experimental device without pre-built files, it probably takes slightly longer.\nAfter reading this guide, you will know:\n\nHow to set basic environment variables\nWhat is a configuration package and how is it structured\n\nTake a look into the operation manual to see how to load and save configuration packages.\n\n\nComputer systems generally use variables on a system level e.g. to store global variables. These variables usually have a name with UPPER_CASE_LETTERS. Also, there is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown. Most of the values have reasonable defaults, but of course the CLUSTER_IP is required when you measure on a cluster and do not want to run only on dummy data.\n\n\n\nFor all other configuration, there is a so-called configuration package. The reason to have a configuration package is to have all configuration files in one place.\nThis is how an example configuration package looks like:\n\nconfigs/: A folder with the configuration files\n\ncluster_config.json: The configuration for the cluster\ndevice_config.toml: The configuration with values related to the device/chip\nnode_config.toml: Some device configuration that is overwritten when a certain node is executed\nrun_config.toml: Run-specific parameters such as the qubits and the target node to calibrate\nspi_config.toml: Defines the spi and groups of couplers\nuser_samplespace.py: Define custom sweeps for the nodes\n\nadditional_files/: A folder with other additional files\n\nmixer_calibration.csv: E.g. the mixer calibration values\n\nwiring_diagrams/: A folder with even more additional files\n\nwiring_diagram.png: E.g. a wiring diagram\n\nconfiguration.meta.toml: The configuration file that describes the structure of the configuration package\n\nThe templates for a full configuration package can be found in tergite_autocalibration/config/templates. There is a .default template to illustrate the general structure of a configuration package. Furthermore, there are some pre-build configuration packages for different kind of setups.\nFor a configuration to be detected by the application, the configuration.meta.toml file should be placed in the root folder of the tergite-autocalibration repository. All filepaths relative to the configuration files have to be able to be resolved. Now, we will go through all the details of these configuration files.\n\n\nThis is the file that always has to be part of the configuration package. It tells the machine where all other configuration files are located, which is crucial to make the automatic loading and saving work. A very simple version of the configuration.meta.toml belonging to the screenshot above would look like this:\npath_prefix = 'configs'\n\n[files]\ncluster_config = 'cluster_config.json'\ndevice_config = 'device_config.toml'\nnode_config = 'node_config.toml'\nrun_config = 'run_config.toml'\nspi_config = 'spi_config.toml'\nuser_samplespace = 'user_samplespace.py'\n\n[misc]\nmixer_calibrations = \"additional_files\"\nwiring_diagrams = \"wiring_diagrams\"\nThe main sections in that .toml file are:\n\npath_prefix: This refers to the folder name into which you would put the other configuration files. If you leaved it empty, this would mean that all configuration files would be inside the same folder with the configuration.meta.toml file.\nThe files section: Here, you put the paths to the configuration files. It can be one or more of the six above. For example, you could also just define cluster_config and device_config and it would be still a valid configuration package. However, maybe during runtime, it would break the code. E.g. if you run without a cluster configuration, it could work fine if you are running a dummy measurement without real hardware, but if you want to measure on real hardware, you would need the cluster configuration. More about the configuration files is described in the sections below for each of the files individually.\nThe misc section: You can add as many folder as you want to that section. Here, we are adding one more folder to the configuration package with additional files. This section is meant to add files like mixer corrections or a wiring diagram, which do not follow a well-defined standard, but might be useful information to be transferred with the configuration package.\n\nSince the configuration.meta.toml file always should reflect how the configuration package looks like, please update it as soon as you add or delete any configuration files from your package.\nNow, in the folder, there are these six configuration files:\n\nCluster configuration\nDevice configuration\nNode configuration\nRun configuration\nSPI configuration (optional, only required for two-qubit calibration)\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nIn the following, there are some more detailed descriptions of what these files mean and contain. More information can also be found in the templates and example configuration files.\n\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Part of a cluster definition\n{\n  \"config_type\": \"quantify_scheduler.backends.qblox_backend.QbloxHardwareCompilationConfig\",\n  \"hardware_description\": {\n    \"clusterA\": {\n      \"instrument_type\": \"Cluster\",\n      \"ref\": \"internal\",\n      \"modules\": {\n        \"2\": {\n          \"instrument_type\": \"QCM_RF\"\n        },\n        \"10\": {\n          \"instrument_type\": \"QRM_RF\"\n        }\n      }\n    }\n  },\n  \"hardware_options\": {\n    \"modulation_frequencies\": {\n      \"q00:mw-q00.01\": {\n        \"lo_freq\": 3946000000.0\n      },\n      ...\n    },\n    \"mixer_corrections\": {\n      \"q00:mw-q00.01\": {\n        \"dc_offset_i\": 0.0,\n        \"dc_offset_q\": 0.0,\n        \"amp_ratio\": 1.0,\n        \"phase_error\": 0.0\n      },\n      ...\n    }\n    },\n  \"connectivity\": {\n    \"graph\": {\n      \"directed\": false,\n      \"multigraph\": false,\n      \"graph\": {},\n      \"nodes\": [\n        {\n          \"instrument_name\": \"clusterA\",\n          \"id\": \"clusterA.module2.complex_output_0\"\n        },\n        ...\n      ],\n      \"links\": [\n        {\n          \"source\": \"clusterA.module2.complex_output_0\",\n          \"target\": \"q00:mw\"\n        },\n        ...\n      ]\n    }\n    }\n}\nThe file in the template package is cluster_configuration.json.\nYou can find more information about the hardware configuration in the documentation of quantify-scheduler\nMigrating old hardware configurations to match quantify-scheduler&gt;=0.18.0\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\n\n[device.qubit.all]\nmeasure.integration_time = 2.5e-6\nmeasure_1.integration_time = 2.5e-6\nmeasure_2.integration_time = 2.5e-6\nrxy.duration = 28e-9\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\n\n\n\nIn this file there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\n\n\n\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the property is a property known to the node. This would load and overwrite the configurations made in the device configuration.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is node_config.toml.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py.\n\n\n\n\nRead about the commandline interface, which contains a chapter about how to load and save configuration packages.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#environment-variables",
    "href": "user-guide/configuration_files.html#environment-variables",
    "title": "Configuration",
    "section": "",
    "text": "Computer systems generally use variables on a system level e.g. to store global variables. These variables usually have a name with UPPER_CASE_LETTERS. Also, there is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown. Most of the values have reasonable defaults, but of course the CLUSTER_IP is required when you measure on a cluster and do not want to run only on dummy data.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#configuration-packages",
    "href": "user-guide/configuration_files.html#configuration-packages",
    "title": "Configuration",
    "section": "",
    "text": "For all other configuration, there is a so-called configuration package. The reason to have a configuration package is to have all configuration files in one place.\nThis is how an example configuration package looks like:\n\nconfigs/: A folder with the configuration files\n\ncluster_config.json: The configuration for the cluster\ndevice_config.toml: The configuration with values related to the device/chip\nnode_config.toml: Some device configuration that is overwritten when a certain node is executed\nrun_config.toml: Run-specific parameters such as the qubits and the target node to calibrate\nspi_config.toml: Defines the spi and groups of couplers\nuser_samplespace.py: Define custom sweeps for the nodes\n\nadditional_files/: A folder with other additional files\n\nmixer_calibration.csv: E.g. the mixer calibration values\n\nwiring_diagrams/: A folder with even more additional files\n\nwiring_diagram.png: E.g. a wiring diagram\n\nconfiguration.meta.toml: The configuration file that describes the structure of the configuration package\n\nThe templates for a full configuration package can be found in tergite_autocalibration/config/templates. There is a .default template to illustrate the general structure of a configuration package. Furthermore, there are some pre-build configuration packages for different kind of setups.\nFor a configuration to be detected by the application, the configuration.meta.toml file should be placed in the root folder of the tergite-autocalibration repository. All filepaths relative to the configuration files have to be able to be resolved. Now, we will go through all the details of these configuration files.\n\n\nThis is the file that always has to be part of the configuration package. It tells the machine where all other configuration files are located, which is crucial to make the automatic loading and saving work. A very simple version of the configuration.meta.toml belonging to the screenshot above would look like this:\npath_prefix = 'configs'\n\n[files]\ncluster_config = 'cluster_config.json'\ndevice_config = 'device_config.toml'\nnode_config = 'node_config.toml'\nrun_config = 'run_config.toml'\nspi_config = 'spi_config.toml'\nuser_samplespace = 'user_samplespace.py'\n\n[misc]\nmixer_calibrations = \"additional_files\"\nwiring_diagrams = \"wiring_diagrams\"\nThe main sections in that .toml file are:\n\npath_prefix: This refers to the folder name into which you would put the other configuration files. If you leaved it empty, this would mean that all configuration files would be inside the same folder with the configuration.meta.toml file.\nThe files section: Here, you put the paths to the configuration files. It can be one or more of the six above. For example, you could also just define cluster_config and device_config and it would be still a valid configuration package. However, maybe during runtime, it would break the code. E.g. if you run without a cluster configuration, it could work fine if you are running a dummy measurement without real hardware, but if you want to measure on real hardware, you would need the cluster configuration. More about the configuration files is described in the sections below for each of the files individually.\nThe misc section: You can add as many folder as you want to that section. Here, we are adding one more folder to the configuration package with additional files. This section is meant to add files like mixer corrections or a wiring diagram, which do not follow a well-defined standard, but might be useful information to be transferred with the configuration package.\n\nSince the configuration.meta.toml file always should reflect how the configuration package looks like, please update it as soon as you add or delete any configuration files from your package.\nNow, in the folder, there are these six configuration files:\n\nCluster configuration\nDevice configuration\nNode configuration\nRun configuration\nSPI configuration (optional, only required for two-qubit calibration)\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nIn the following, there are some more detailed descriptions of what these files mean and contain. More information can also be found in the templates and example configuration files.\n\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Part of a cluster definition\n{\n  \"config_type\": \"quantify_scheduler.backends.qblox_backend.QbloxHardwareCompilationConfig\",\n  \"hardware_description\": {\n    \"clusterA\": {\n      \"instrument_type\": \"Cluster\",\n      \"ref\": \"internal\",\n      \"modules\": {\n        \"2\": {\n          \"instrument_type\": \"QCM_RF\"\n        },\n        \"10\": {\n          \"instrument_type\": \"QRM_RF\"\n        }\n      }\n    }\n  },\n  \"hardware_options\": {\n    \"modulation_frequencies\": {\n      \"q00:mw-q00.01\": {\n        \"lo_freq\": 3946000000.0\n      },\n      ...\n    },\n    \"mixer_corrections\": {\n      \"q00:mw-q00.01\": {\n        \"dc_offset_i\": 0.0,\n        \"dc_offset_q\": 0.0,\n        \"amp_ratio\": 1.0,\n        \"phase_error\": 0.0\n      },\n      ...\n    }\n    },\n  \"connectivity\": {\n    \"graph\": {\n      \"directed\": false,\n      \"multigraph\": false,\n      \"graph\": {},\n      \"nodes\": [\n        {\n          \"instrument_name\": \"clusterA\",\n          \"id\": \"clusterA.module2.complex_output_0\"\n        },\n        ...\n      ],\n      \"links\": [\n        {\n          \"source\": \"clusterA.module2.complex_output_0\",\n          \"target\": \"q00:mw\"\n        },\n        ...\n      ]\n    }\n    }\n}\nThe file in the template package is cluster_configuration.json.\nYou can find more information about the hardware configuration in the documentation of quantify-scheduler\nMigrating old hardware configurations to match quantify-scheduler&gt;=0.18.0\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\n\n[device.qubit.all]\nmeasure.integration_time = 2.5e-6\nmeasure_1.integration_time = 2.5e-6\nmeasure_2.integration_time = 2.5e-6\nrxy.duration = 28e-9\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\n\n\n\nIn this file there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\n\n\n\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the property is a property known to the node. This would load and overwrite the configurations made in the device configuration.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is node_config.toml.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#next-steps",
    "href": "user-guide/configuration_files.html#next-steps",
    "title": "Configuration",
    "section": "",
    "text": "Read about the commandline interface, which contains a chapter about how to load and save configuration packages.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "nodes/resonator_spectroscopy_node.html",
    "href": "nodes/resonator_spectroscopy_node.html",
    "title": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis",
    "section": "",
    "text": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis\nResonator spectroscopy is a powerful tool for studying the interaction between qubits and resonators. By probing the resonator’s response across various frequencies, we can extract critical parameters, such as the resonant frequency and Q-factor, which are essential for optimizing qubit-resonator interactions.\n\nClass: ResonatorSpectroscopyNode\nThe Resonator_Spectroscopy class is designed to conduct resonator spectroscopy for transmon qubits. It takes as input a dictionary of transmon qubits and their respective states.\n\nMethod: schedule\nThe schedule method generates a schedule to perform resonator spectroscopy. This process includes:\n\nClock Initialization: Initializes the clocks for each qubit based on the specified qubit state.\nQubit Reset: Resets the qubit to a known state.\nFrequency Probing: Applies a square pulse at various frequencies.\nSignal Measurement: Measures the response signal, capturing data on the resonator’s behavior.\n\n\n\n\nClass: ResonatorSpectroscopyQubitAnalysis\nThe ResonatorSpectroscopyQubitAnalysis class is used for analyzing resonator spectroscopy data, enabling the extraction of the resonant frequency and Q-factor. This class takes as parameters:\n\nqubit_name: A string representing the qubit under measurement.\nredis_fields: The directory for data storage.\n\n\nMethod: analyse_qubit\nThe analyse_qubit method processes and fits the data to determine the resonator’s resonant frequency and loaded Q-factor. The analyse_qubitfit should resemble a negative Gaussian distribution.\n\n\n\nOutput: xarray.Dataset\nThe dataset returned by this analysis is an xarray.Dataset, which includes:\n\nFrequency Sweep Data: The set of frequencies used during the sweep.\nTransmission Response: The measured response of the resonator at each frequency.\n\nThis dataset provides essential insights into the resonator’s properties, allowing for precise tuning of qubit-resonator interactions.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy"
    ]
  },
  {
    "objectID": "nodes/qubit_spectroscopy_node.html",
    "href": "nodes/qubit_spectroscopy_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "Qubit spectroscopy is a vital technique for identifying qubit resonance frequencies. By applying a probing signal to a qubit at various frequencies and measuring the response, we can accurately locate resonance frequencies and optimize qubit operation. In this node, both qubit frequecies for both 01 and 12 can be attained depending on the initial qubit state.\n\n\nThe TwoTonesMultidimMeasurement class facilitates the creation of schedules for qubit spectroscopy experiments. It supports multi-qubit spectroscopy, enabling parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nFrequency Sweeping: Iteratively adjusts the probing frequency and amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nspec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each qubit.\nspec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the probing pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe QubitSpectroscopyMultidimAnalysis class analyzes the results of qubit spectroscopy experiments. The resonance peak is identified, enabling the determination of resonance frequencies.\n\n\nThe analyse_qubit method processes the spectroscopy data to extract key parameters:\n\nQubit Frequency: The frequency at which the qubit exhibits a resonance peak.\nOptimal Spectroscopy Amplitude: The amplitude yielding the strongest response.\n\nSteps:\n\nExtract the relevant coordinates (frequencies and amplitudes) from the dataset.\nIdentify the resonance peak.\nValidate the peak based on prominence and width criteria.\n\nReturns:\n\nA list containing the qubit frequency and the optimal spectroscopy amplitude.\n\n\n\n\nDetermines if a resonance peak exists in the data using statistical filters and peak detection.\nParameters:\n\nx (array): Data array to evaluate.\nprom_coef (float): Prominence coefficient.\nwid_coef (float): Width coefficient.\noutlier_median (float): Threshold for filtering outliers.\n\nReturns:\n\nA boolean indicating whether a peak is present.",
    "crumbs": [
      "Home",
      "Node Library",
      "Qubit spectroscopy"
    ]
  },
  {
    "objectID": "nodes/qubit_spectroscopy_node.html#qubit-spectroscopy-calibration-and-analysis",
    "href": "nodes/qubit_spectroscopy_node.html#qubit-spectroscopy-calibration-and-analysis",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "Qubit spectroscopy is a vital technique for identifying qubit resonance frequencies. By applying a probing signal to a qubit at various frequencies and measuring the response, we can accurately locate resonance frequencies and optimize qubit operation. In this node, both qubit frequecies for both 01 and 12 can be attained depending on the initial qubit state.\n\n\nThe TwoTonesMultidimMeasurement class facilitates the creation of schedules for qubit spectroscopy experiments. It supports multi-qubit spectroscopy, enabling parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nFrequency Sweeping: Iteratively adjusts the probing frequency and amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nspec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each qubit.\nspec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the probing pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe QubitSpectroscopyMultidimAnalysis class analyzes the results of qubit spectroscopy experiments. The resonance peak is identified, enabling the determination of resonance frequencies.\n\n\nThe analyse_qubit method processes the spectroscopy data to extract key parameters:\n\nQubit Frequency: The frequency at which the qubit exhibits a resonance peak.\nOptimal Spectroscopy Amplitude: The amplitude yielding the strongest response.\n\nSteps:\n\nExtract the relevant coordinates (frequencies and amplitudes) from the dataset.\nIdentify the resonance peak.\nValidate the peak based on prominence and width criteria.\n\nReturns:\n\nA list containing the qubit frequency and the optimal spectroscopy amplitude.\n\n\n\n\nDetermines if a resonance peak exists in the data using statistical filters and peak detection.\nParameters:\n\nx (array): Data array to evaluate.\nprom_coef (float): Prominence coefficient.\nwid_coef (float): Width coefficient.\noutlier_median (float): Threshold for filtering outliers.\n\nReturns:\n\nA boolean indicating whether a peak is present.",
    "crumbs": [
      "Home",
      "Node Library",
      "Qubit spectroscopy"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This guide contains some information on how to get started with the automatic calibration. Please consider also reading the README.md file in the git repository for more information.\n\n\nThe automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.\n\n\n\nThe first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.12\nconda create -n tac python=3.12 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\nConfiguration files in case you are interested on how to configure the application and run the first experiments.\nDeveloper guide in case you would like to start developing features for the automatic calibration.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#prerequisites",
    "href": "getting_started.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "The automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#installation",
    "href": "getting_started.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "The first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.12\nconda create -n tac python=3.12 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\nConfiguration files in case you are interested on how to configure the application and run the first experiments.\nDeveloper guide in case you would like to start developing features for the automatic calibration.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "developer_guide.html",
    "href": "developer_guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "This and the following sections provide information on how to develop code in tergite-autocalibration.\n\n\nConsider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e \".[test,dev]\"\nthis will install the additional packages for developing code and running tests\n\n\n\nWe use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case\n\n\n\n\nThere are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.\n\n\n\n\nPlease read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML\n\n\n\n\nAs you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#additional-installations",
    "href": "developer_guide.html#additional-installations",
    "title": "Developer Guide",
    "section": "",
    "text": "Consider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e \".[test,dev]\"\nthis will install the additional packages for developing code and running tests",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#naming-convention-and-style",
    "href": "developer_guide.html#naming-convention-and-style",
    "title": "Developer Guide",
    "section": "",
    "text": "We use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#ide-preloaded-settings",
    "href": "developer_guide.html#ide-preloaded-settings",
    "title": "Developer Guide",
    "section": "",
    "text": "There are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#things-to-do-before-a-commit",
    "href": "developer_guide.html#things-to-do-before-a-commit",
    "title": "Developer Guide",
    "section": "",
    "text": "Please read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#next-steps",
    "href": "developer_guide.html#next-steps",
    "title": "Developer Guide",
    "section": "",
    "text": "As you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html",
    "href": "developer-guide/writing_documentation.html",
    "title": "Writing documentation",
    "section": "",
    "text": "Writing good documentation is very important for new people to understand how to use things. Often, we do not have time to write good guides, because we are having deadlines or other things to do that are more interesting and less boring than writing down what we already seem to know so well.\nThis guide is a small guide on how to write a good guide. It does not aim to provide an explanation on how to write the perfect documentation, because these resources already exist on the internet. However, it should make at least curious on how to write documentation with some short general rules:\n\nThink like the reader\nGo step by step\nBe precise\nFollow standards\nDiscuss\n\nIn the second part of the tutorial, we will focus on how to write documentation especially for this project.\nSo, scroll down if you want to start reading there.\n\n\nWhen you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.\n\n\n\n\n\nIn this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.\n\n\n\n\nWhen you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "href": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "title": "Writing documentation",
    "section": "",
    "text": "When you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "href": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "title": "Writing documentation",
    "section": "",
    "text": "In this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#next-steps",
    "href": "developer-guide/writing_documentation.html#next-steps",
    "title": "Writing documentation",
    "section": "",
    "text": "When you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/logging.html",
    "href": "developer-guide/logging.html",
    "title": "Logging",
    "section": "",
    "text": "This page is about logging and will guide through a couple of questions such as:\n\nHow do logs end up in the console and in the log file?\nWhere are logs stored?\nHow is the logger used inside the code?\nWhat logging levels are there?\n\nLogging is a minor aspect of the application, but if there is some consistency in the way logs are handled, it can make the debugging much faster.\n\n\nIn Python there are several ways to log messages. The easiest one is probably to put a print() statement into the code and see the outcome in the console. There are a couple of way such as sys.stdout.write() to do logging closer to the system. Finally, a very sophisticated way to do logging is offered with the built-in logging module.\nWith the logging package, it is easy to register an own logger class and to implement functionalities such as logging to the console and a file at the same time. Also, the logging package offers levels to prioritize messages in the logging. With these levels it is possible to have a very high verbosity in the logging during development and only show critical errors or warning during production. More about levels below in the section about logging levels.\nIn the automatic calibration, there is an extended version of the logger in the logging package. It can be found in utils/logging/__init__.py. When you want to use the logger inside the code, you will have to import the logger:\nfrom tergite_autocalibration.utils.logging import logger\n\nlogger.info(\"Hello world\")\nThis will create the logger and print a message on the info level.\n\n\n\nThe logging package offers different levels such as notset=0 debug=10, info=20, warning=30, error=40, and critical=50. If the level is set to e.g. 30, all messages with a lower logging level such as debug or info will be suppressed.\nWhen you are having a lot of external packages such as qcodes, they might log a lot of messages on the info level, which are still very detailed. This is why the autocalibration has a level itself, which is on 25 and called status. Here, you can see how this looks in practice:\nfrom tergite_autocalibration.utils.logging import logger\n\nlogger.warning(\"This is a very important log message\") # level: 30\nlogger.status(\"This is an important log message\")      # level: 25\nlogger.info(\"This is a less important log message\")    # level: 20\nAs a rule of thumb, use logger.status() for important messages such as there is a node to be calibrated. Use the logger.info() for less important messages such as printing the fit result of an analysis function. In case you want to print very low level messages, such as raw results or indications where you enter a function while debugging, please use logger.debug(). And for warnings and errors, use the respective log levels.\nYou can set different log levels for the console and file output by setting the environmental variables in the .env file.\nSTDOUT_LOG_LEVEL='25'\nFILE_LOG_LEVEL='10'\nThese values above are the default. The effect will be that the application will write a very detailed log to the log file. In the console prints, there will be only status, warning and error messages.\n\n\n\nThe logs are stored in a directory that is defined by the data directory in the environment variable DATA_DIR. Usually, there should be no need to define the DATA_DIR variable as the application will automatically create a folder called /out on the root level of the repository. Inside the data directory there will be created a folder for each day in the format YYY-MM-DD when one starts a measurement.\nFor each run of the calibration or other command, there is going to be created another subfolder inside the folder of the day. This folder follows the pattern HH-MM-SS_STATUS-node_name meaning that is starts with a timestamp, followed by the application status and the node, which is the target to calibrate. For the status there are the following options:\n\nACTIVE: While your script is running, this indicates that inside the folder, there will be active read and write operations.\nSUCCESS: In case the calibration terminated successfully. This does not mean that it has found the right calibration values. It just means that it did not crash at some point in between.\nFAILED: In case there was an error.\n\nThese folder names are changing automatically while the application is running or shutting down, so, please do not change the folder names manually during a calibration.\n\n\nThere are two special occasions when logs are taken that do not fall into the categories above.\nThe first of them is the default folder. This folder is used for all logs where the application is used either the first time or when the application is in a state without any configuration. The default log will just be appended and create one long log file.\nLogs can also be intentionally routed to the default log directory by adding a suppress_logging decorator to a commandline endpoint. This should be added below the typer command decorator.\nimport typer\nfrom tergite_autocalibration.utils.logging.decorators import suppress_logging\n\nmy_typer_cli = typer.Typer()\n\n@my_typer_cli.command(help=\"Example command\")\n@suppress_logging\ndef endpoint():\n    \"\"\"Endpoint for the cli\"\"\"\n    pass\nPlease note that this method is only meant to route logging within typer endpoints. If you want to change the verbosity of your logs, please read the section above.\nThe other special folder to save logs is the pytest folder. It contains the logs from running the unit tests.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Logging"
    ]
  },
  {
    "objectID": "developer-guide/logging.html#general-logging-to-console-and-file",
    "href": "developer-guide/logging.html#general-logging-to-console-and-file",
    "title": "Logging",
    "section": "",
    "text": "In Python there are several ways to log messages. The easiest one is probably to put a print() statement into the code and see the outcome in the console. There are a couple of way such as sys.stdout.write() to do logging closer to the system. Finally, a very sophisticated way to do logging is offered with the built-in logging module.\nWith the logging package, it is easy to register an own logger class and to implement functionalities such as logging to the console and a file at the same time. Also, the logging package offers levels to prioritize messages in the logging. With these levels it is possible to have a very high verbosity in the logging during development and only show critical errors or warning during production. More about levels below in the section about logging levels.\nIn the automatic calibration, there is an extended version of the logger in the logging package. It can be found in utils/logging/__init__.py. When you want to use the logger inside the code, you will have to import the logger:\nfrom tergite_autocalibration.utils.logging import logger\n\nlogger.info(\"Hello world\")\nThis will create the logger and print a message on the info level.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Logging"
    ]
  },
  {
    "objectID": "developer-guide/logging.html#logging-levels",
    "href": "developer-guide/logging.html#logging-levels",
    "title": "Logging",
    "section": "",
    "text": "The logging package offers different levels such as notset=0 debug=10, info=20, warning=30, error=40, and critical=50. If the level is set to e.g. 30, all messages with a lower logging level such as debug or info will be suppressed.\nWhen you are having a lot of external packages such as qcodes, they might log a lot of messages on the info level, which are still very detailed. This is why the autocalibration has a level itself, which is on 25 and called status. Here, you can see how this looks in practice:\nfrom tergite_autocalibration.utils.logging import logger\n\nlogger.warning(\"This is a very important log message\") # level: 30\nlogger.status(\"This is an important log message\")      # level: 25\nlogger.info(\"This is a less important log message\")    # level: 20\nAs a rule of thumb, use logger.status() for important messages such as there is a node to be calibrated. Use the logger.info() for less important messages such as printing the fit result of an analysis function. In case you want to print very low level messages, such as raw results or indications where you enter a function while debugging, please use logger.debug(). And for warnings and errors, use the respective log levels.\nYou can set different log levels for the console and file output by setting the environmental variables in the .env file.\nSTDOUT_LOG_LEVEL='25'\nFILE_LOG_LEVEL='10'\nThese values above are the default. The effect will be that the application will write a very detailed log to the log file. In the console prints, there will be only status, warning and error messages.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Logging"
    ]
  },
  {
    "objectID": "developer-guide/logging.html#log-directories",
    "href": "developer-guide/logging.html#log-directories",
    "title": "Logging",
    "section": "",
    "text": "The logs are stored in a directory that is defined by the data directory in the environment variable DATA_DIR. Usually, there should be no need to define the DATA_DIR variable as the application will automatically create a folder called /out on the root level of the repository. Inside the data directory there will be created a folder for each day in the format YYY-MM-DD when one starts a measurement.\nFor each run of the calibration or other command, there is going to be created another subfolder inside the folder of the day. This folder follows the pattern HH-MM-SS_STATUS-node_name meaning that is starts with a timestamp, followed by the application status and the node, which is the target to calibrate. For the status there are the following options:\n\nACTIVE: While your script is running, this indicates that inside the folder, there will be active read and write operations.\nSUCCESS: In case the calibration terminated successfully. This does not mean that it has found the right calibration values. It just means that it did not crash at some point in between.\nFAILED: In case there was an error.\n\nThese folder names are changing automatically while the application is running or shutting down, so, please do not change the folder names manually during a calibration.\n\n\nThere are two special occasions when logs are taken that do not fall into the categories above.\nThe first of them is the default folder. This folder is used for all logs where the application is used either the first time or when the application is in a state without any configuration. The default log will just be appended and create one long log file.\nLogs can also be intentionally routed to the default log directory by adding a suppress_logging decorator to a commandline endpoint. This should be added below the typer command decorator.\nimport typer\nfrom tergite_autocalibration.utils.logging.decorators import suppress_logging\n\nmy_typer_cli = typer.Typer()\n\n@my_typer_cli.command(help=\"Example command\")\n@suppress_logging\ndef endpoint():\n    \"\"\"Endpoint for the cli\"\"\"\n    pass\nPlease note that this method is only meant to route logging within typer endpoints. If you want to change the verbosity of your logs, please read the section above.\nThe other special folder to save logs is the pytest folder. It contains the logs from running the unit tests.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Logging"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html",
    "href": "developer-guide/unit_tests.html",
    "title": "Unit tests",
    "section": "",
    "text": "When testing software, there are several ways to test whether it is working. Amongst others such as system integration tests or usability scores, there are unit tests. A unit test is meant to confirm whether a small unit of the code is working such as a function or a class. The idea behind unit tests though, is to create tiny little tests for each function, that checks how it handles:\n\nNormal case: The things that you would expect a function to do e.g. if you have addition, and you add natural numbers.\nEdge cases: In the example with the addition this would be e.g. whether it correctly adds zero or would subtract if there is a negative number.\nFail cases: Let us say you have addition, and you are adding a number such as 20 with the string “hello”. This should fail.\n\nHaving thought about the test cases and possible scenarios will help to get a better understanding what the code does, also, it will make the code more robust. In our code base we are having a pipeline that will automatically run all tests as soon as someone wants to merge to the common branches. There are two locations where tests are stored:\n\nIn the folder for tests: This is a folder on the main level of the repository where more general tests for the whole framework go.\nIn the folder of each node: This is to test only the node itself. The tests are added directly to the node module.\n\nSince it happens more often that one will write tests for the node itself, in the following, there will be a section explaining how to do it on an example node.\n\n\nTo run the unit tests, we are using the test framework pytest. If you prefer writing your tests with the Python built-in framework unittest, you are free to do so. Your tests will be also recognized by pytest, but to keep a standard, we recommend using pytest.\nBefore running your tests, please make sure that there is a redis instance running on port 6378. You can start a redis instance with:\nredis-server --port 6378 {--daemonize yes}\nOptionally, you can add the --daemonize yes parameter. This will make the redis instance run in the background. If it does not run on your user, try running it with sudo rights.\nAssuming you open a terminal in the root directory of the repository, you can run the pytests for the whole autocalibration package.\npytest tergite_autocalibration\nIf you want to test only a subset of tests in a specific folder you can it as well by running:\npytest tergite_autocalibration/lib/nodes/readout/resonator_spectroscopy\nThis would only run the tests for resonator spectroscopy. You can even run the tests only for a single function, by using:\npytest tergite_autocalibration/tests/test_formatting.py::test_license_headers\nThis would only run the function that checks whether all files have a license header.\n\n\n\nThese instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\nfrom tergite_autocalibration.lib.nodes.schedule_node import ScheduleNode\n\n\ndef test_canCreateCorrectType():\n  c = CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n  assert isinstance(c, CZParametrizationFixDurationNode)\n  assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\n\n\ndef test_CanGetQubitsFromCouplers():\n  c = CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n  assert c.couplers == [\"q14_q15\"]\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\n\nimport pytest\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n  with pytest.raises(ValueError):\n    CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\nimport os\nimport pytest\nfrom pathlib import Path\nimport xarray as xr\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis\n)\n\n\ndef test_canGetMaxFromQ1(setup_good_data):\n  d14, d15, freqs, amps = setup_good_data\n  c = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  result = c.run_fitting()\n  indexBestFreq = np.where(freqs == result[0])[0]\n  indexBestAmp = np.where(amps == result[1])[0]\n  assert indexBestFreq[0] == 9\n  assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n  d14, d15, freqs, amps = setup_good_data\n  c = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  result = c.run_fitting()\n  indexBestFreq = np.where(freqs == result[0])[0]\n  indexBestAmp = np.where(amps == result[1])[0]\n  assert indexBestFreq[0] == 10\n  assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\nimport os\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis\n)\n\n\ndef test_canPlotBad(setup_bad_data):\n  matplotlib.use(\"Agg\")\n  d14, d15, freqs, amps = setup_bad_data\n  c14 = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  result = c14.run_fitting()\n\n  figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n  # Remove the file if it already exists\n  if os.path.exists(figure_path):\n    os.remove(figure_path)\n\n  fig, ax = plt.subplots(figsize=(15, 7), num=1)\n  plt.Axes\n  c14.plotter(ax)\n  fig.savefig(figure_path)\n  plt.close()\n\n  assert os.path.exists(figure_path)\n  from PIL import Image\n\n  with Image.open(figure_path) as img:\n    assert img.format == \"PNG\", \"File should be a PNG image\"\n\n  c15 = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  result = c15.run_fitting()\n\n  figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n  # Remove the file if it already exists\n  if os.path.exists(figure_path):\n    os.remove(figure_path)\n\n  fig, ax = plt.subplots(figsize=(15, 7), num=1)\n  plt.Axes\n  c15.plotter(ax)\n  fig.savefig(figure_path)\n  plt.close()\n\n  assert os.path.exists(figure_path)\n  from PIL import Image\n\n  with Image.open(figure_path) as img:\n    assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\nimport pytest\nfrom pathlib import Path\nimport xarray as xr\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis,\n  CombinedFrequencyVsAmplitudeAnalysis\n)\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  q15Res = q15Ana.run_fitting()\n  return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n  q14Res, q15Res = setup_bad_data\n  c = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n  r = c.are_frequencies_compatible()\n  assert r == False\n  r = c.are_amplitudes_compatible()\n  assert r == False\n  r = c.are_two_qubits_compatible()\n  assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\nfrom pathlib import Path\nimport xarray as xr\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis,\n  CombinedFrequencyVsAmplitudeAnalysis\n)\n\n\ndef setup_data():\n  # It should be a single dataset, but we do not have one yet, so we loop over existing files\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  q15Res = q15Ana.run_fitting()\n  c1 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(\n    d14, freqs_bad, amps_bad\n  )\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(\n    d15, freqs_bad, amps_bad\n  )\n  q15Res = q15Ana.run_fitting()\n  c2 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  dataset_path = (\n          Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n  )\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs_2, amps_2)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs_2, amps_2)\n  q15Res = q15Ana.run_fitting()\n  c3 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n  return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.\n\n\n\n\nWhen running unit tests, the test framework itself and the way tests are executed can cause problems that make the unit tests fail. Hence, it is important to understand how pytest works to get a grip on how to debug failing tests efficiently.\nIn the beginning of each test run, pytest will try to import all necessary modules. If there are any “Unresolved reference” errors, the tests will even not start to run.\n\n\nTests might even indirectly affect the outcome of another test. As an example, there might be a test, which changes some global state in the application. Let us say the default value for the global variable VAR_1 is 0, but one of the test changes sets the global variable VAR_1 to 1. Now, all following tests will read 1 when they get VAR_1 from the environment.\nSometimes though, it might be necessary to change to global variable of one test. To handle these situations, the test framework has implemented some decorators, which intend to freeze the environment variables. The most simple way to freeze the state of the environment and then e.g. set variables inside the test function is with the @preserve_os_env decorator.\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import preserve_os_env\n\n\n@preserve_os_env\ndef test_my_function_that_sets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  os.environ[\"VAR_1\"] = \"1\"\n  new_var1 = os.environ[\"VAR_1\"]\n  assert int(var1) + 1 == int(new_var1)\n\n\ndef test_my_function_that_only_gets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  assert int(var1) == 0\nHere, if we had run the first test without the @preserve_os_env decorator, the second test would fail, because the variable would be set to 1 in the first test and does not have the default value 0.\nAnother decorator in that regard is the @with_os_env decorator, which takes as an input the dictionary of values that environment should hold. It is just a way that - similarly to a fixture - simplifies the way things are set up without having too many os.environ calls inside the code. For example, you want to change the value of an environmental variable in exactly one test function, you can add:\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import with_os_env\n\n\n@with_os_env({\"VAR_1\": \"1\"})\ndef test_my_function_that_needs_the_os_variables_changed():\n  var1 = os.environ[\"VAR_1\"]\n  assert var1 == \"ABC\"\nThis will freeze the current state of the environment, replace the variable VAR_1 with “ABC” and after the test will cleanup the environment and load the previous values.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#running-unit-tests",
    "href": "developer-guide/unit_tests.html#running-unit-tests",
    "title": "Unit tests",
    "section": "",
    "text": "To run the unit tests, we are using the test framework pytest. If you prefer writing your tests with the Python built-in framework unittest, you are free to do so. Your tests will be also recognized by pytest, but to keep a standard, we recommend using pytest.\nBefore running your tests, please make sure that there is a redis instance running on port 6378. You can start a redis instance with:\nredis-server --port 6378 {--daemonize yes}\nOptionally, you can add the --daemonize yes parameter. This will make the redis instance run in the background. If it does not run on your user, try running it with sudo rights.\nAssuming you open a terminal in the root directory of the repository, you can run the pytests for the whole autocalibration package.\npytest tergite_autocalibration\nIf you want to test only a subset of tests in a specific folder you can it as well by running:\npytest tergite_autocalibration/lib/nodes/readout/resonator_spectroscopy\nThis would only run the tests for resonator spectroscopy. You can even run the tests only for a single function, by using:\npytest tergite_autocalibration/tests/test_formatting.py::test_license_headers\nThis would only run the function that checks whether all files have a license header.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "href": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "title": "Unit tests",
    "section": "",
    "text": "These instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\nfrom tergite_autocalibration.lib.nodes.schedule_node import ScheduleNode\n\n\ndef test_canCreateCorrectType():\n  c = CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n  assert isinstance(c, CZParametrizationFixDurationNode)\n  assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\n\n\ndef test_CanGetQubitsFromCouplers():\n  c = CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n  assert c.couplers == [\"q14_q15\"]\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\n\nimport pytest\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.node import CZParametrizationFixDurationNode\n\n\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n  with pytest.raises(ValueError):\n    CZParametrizationFixDurationNode(\"cz_char_fixCurrent\", couplers=[\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\nimport os\nimport pytest\nfrom pathlib import Path\nimport xarray as xr\n\n\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis\n)\n\n\ndef test_canGetMaxFromQ1(setup_good_data):\n  d14, d15, freqs, amps = setup_good_data\n  c = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  result = c.run_fitting()\n  indexBestFreq = np.where(freqs == result[0])[0]\n  indexBestAmp = np.where(amps == result[1])[0]\n  assert indexBestFreq[0] == 9\n  assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n  d14, d15, freqs, amps = setup_good_data\n  c = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  result = c.run_fitting()\n  indexBestFreq = np.where(freqs == result[0])[0]\n  indexBestAmp = np.where(amps == result[1])[0]\n  assert indexBestFreq[0] == 10\n  assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\nimport os\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis\n)\n\n\ndef test_canPlotBad(setup_bad_data):\n  matplotlib.use(\"Agg\")\n  d14, d15, freqs, amps = setup_bad_data\n  c14 = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  result = c14.run_fitting()\n\n  figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n  # Remove the file if it already exists\n  if os.path.exists(figure_path):\n    os.remove(figure_path)\n\n  fig, ax = plt.subplots(figsize=(15, 7), num=1)\n  plt.Axes\n  c14.plotter(ax)\n  fig.savefig(figure_path)\n  plt.close()\n\n  assert os.path.exists(figure_path)\n  from PIL import Image\n\n  with Image.open(figure_path) as img:\n    assert img.format == \"PNG\", \"File should be a PNG image\"\n\n  c15 = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  result = c15.run_fitting()\n\n  figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n  # Remove the file if it already exists\n  if os.path.exists(figure_path):\n    os.remove(figure_path)\n\n  fig, ax = plt.subplots(figsize=(15, 7), num=1)\n  plt.Axes\n  c15.plotter(ax)\n  fig.savefig(figure_path)\n  plt.close()\n\n  assert os.path.exists(figure_path)\n  from PIL import Image\n\n  with Image.open(figure_path) as img:\n    assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\nimport pytest\nfrom pathlib import Path\nimport xarray as xr\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis,\n  CombinedFrequencyVsAmplitudeAnalysis\n)\n\n\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  q15Res = q15Ana.run_fitting()\n  return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n  q14Res, q15Res = setup_bad_data\n  c = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n  r = c.are_frequencies_compatible()\n  assert r == False\n  r = c.are_amplitudes_compatible()\n  assert r == False\n  r = c.are_two_qubits_compatible()\n  assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\nfrom pathlib import Path\nimport xarray as xr\n\nfrom tergite_autocalibration.lib.nodes.coupler.cz_parametrization.analysis import (\n  FrequencyVsAmplitudeQ1Analysis,\n  FrequencyVsAmplitudeQ2Analysis,\n  CombinedFrequencyVsAmplitudeAnalysis\n)\n\n\ndef setup_data():\n  # It should be a single dataset, but we do not have one yet, so we loop over existing files\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs, amps)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs, amps)\n  q15Res = q15Ana.run_fitting()\n  c1 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(\n    d14, freqs_bad, amps_bad\n  )\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(\n    d15, freqs_bad, amps_bad\n  )\n  q15Res = q15Ana.run_fitting()\n  c2 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  dataset_path = (\n          Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n  )\n  print(dataset_path)\n  ds = xr.open_dataset(dataset_path)\n  ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n  d14 = ds.yq14.to_dataset()\n  d15 = ds.yq15.to_dataset()\n  d14.yq14.attrs[\"qubit\"] = \"q14\"\n  d15.yq15.attrs[\"qubit\"] = \"q15\"\n  freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n  amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n  q14Ana = FrequencyVsAmplitudeQ1Analysis(d14, freqs_2, amps_2)\n  q14Res = q14Ana.run_fitting()\n  q15Ana = FrequencyVsAmplitudeQ2Analysis(d15, freqs_2, amps_2)\n  q15Res = q15Ana.run_fitting()\n  c3 = CombinedFrequencyVsAmplitudeAnalysis(q14Res, q15Res)\n\n  list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n  return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#advanced-topics-on-unit-tests",
    "href": "developer-guide/unit_tests.html#advanced-topics-on-unit-tests",
    "title": "Unit tests",
    "section": "",
    "text": "When running unit tests, the test framework itself and the way tests are executed can cause problems that make the unit tests fail. Hence, it is important to understand how pytest works to get a grip on how to debug failing tests efficiently.\nIn the beginning of each test run, pytest will try to import all necessary modules. If there are any “Unresolved reference” errors, the tests will even not start to run.\n\n\nTests might even indirectly affect the outcome of another test. As an example, there might be a test, which changes some global state in the application. Let us say the default value for the global variable VAR_1 is 0, but one of the test changes sets the global variable VAR_1 to 1. Now, all following tests will read 1 when they get VAR_1 from the environment.\nSometimes though, it might be necessary to change to global variable of one test. To handle these situations, the test framework has implemented some decorators, which intend to freeze the environment variables. The most simple way to freeze the state of the environment and then e.g. set variables inside the test function is with the @preserve_os_env decorator.\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import preserve_os_env\n\n\n@preserve_os_env\ndef test_my_function_that_sets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  os.environ[\"VAR_1\"] = \"1\"\n  new_var1 = os.environ[\"VAR_1\"]\n  assert int(var1) + 1 == int(new_var1)\n\n\ndef test_my_function_that_only_gets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  assert int(var1) == 0\nHere, if we had run the first test without the @preserve_os_env decorator, the second test would fail, because the variable would be set to 1 in the first test and does not have the default value 0.\nAnother decorator in that regard is the @with_os_env decorator, which takes as an input the dictionary of values that environment should hold. It is just a way that - similarly to a fixture - simplifies the way things are set up without having too many os.environ calls inside the code. For example, you want to change the value of an environmental variable in exactly one test function, you can add:\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import with_os_env\n\n\n@with_os_env({\"VAR_1\": \"1\"})\ndef test_my_function_that_needs_the_os_variables_changed():\n  var1 = os.environ[\"VAR_1\"]\n  assert var1 == \"ABC\"\nThis will freeze the current state of the environment, replace the variable VAR_1 with “ABC” and after the test will cleanup the environment and load the previous values.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "nodes/coupler_spectroscopy.html",
    "href": "nodes/coupler_spectroscopy.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe CouplerSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerSpectroscopy performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the coupler reconator spectroscopy node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyAnalysisForCouplerSpectroscopy stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher."
  },
  {
    "objectID": "nodes/coupler_spectroscopy.html#coupler-spectroscopy",
    "href": "nodes/coupler_spectroscopy.html#coupler-spectroscopy",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe CouplerSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerSpectroscopy performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the coupler reconator spectroscopy node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyAnalysisForCouplerSpectroscopy stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher."
  },
  {
    "objectID": "nodes/coupler_resonator_spectroscopy.html",
    "href": "nodes/coupler_resonator_spectroscopy.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler resonator spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe CouplerResonatorSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerResonatorSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerResonatorSpectroscopy performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node."
  },
  {
    "objectID": "nodes/coupler_resonator_spectroscopy.html#coupler-resonator-spectroscopy",
    "href": "nodes/coupler_resonator_spectroscopy.html#coupler-resonator-spectroscopy",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler resonator spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe CouplerResonatorSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerResonatorSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerResonatorSpectroscopy performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node."
  },
  {
    "objectID": "nodes/coupler_spectroscopy_node.html",
    "href": "nodes/coupler_spectroscopy_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe CouplerSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerSpectroscopy performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the coupler reconator spectroscopy node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyAnalysisForCouplerSpectroscopy stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher."
  },
  {
    "objectID": "nodes/coupler_spectroscopy_node.html#coupler-spectroscopy",
    "href": "nodes/coupler_spectroscopy_node.html#coupler-spectroscopy",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe CouplerSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerSpectroscopy performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the coupler reconator spectroscopy node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyAnalysisForCouplerSpectroscopy stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher."
  },
  {
    "objectID": "nodes/coupler_resonator_spectroscopy_node.html",
    "href": "nodes/coupler_resonator_spectroscopy_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler resonator spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe CouplerResonatorSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerResonatorSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerResonatorSpectroscopy performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node."
  },
  {
    "objectID": "nodes/coupler_resonator_spectroscopy_node.html#coupler-resonator-spectroscopy",
    "href": "nodes/coupler_resonator_spectroscopy_node.html#coupler-resonator-spectroscopy",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The coupler resonator spectroscopy is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe CouplerResonatorSpectroscopyNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe CouplerResonatorSpectroscopyAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitAnalysisForCouplerResonatorSpectroscopy performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node."
  },
  {
    "objectID": "nodes/resonator_spectroscopy_vs_current_node.html",
    "href": "nodes/resonator_spectroscopy_vs_current_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The qubit spectroscopy as a function of the coupler current is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe ResonatorSpectroscopyVsCurrentNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe ResonatorSpectroscopyVsCurrentCouplerAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe ResonatorSpectroscopyVsCurrentQubitAnalysis performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy vs current"
    ]
  },
  {
    "objectID": "nodes/resonator_spectroscopy_vs_current_node.html#resonator-spectroscopy-as-a-function-of-the-coupler-current",
    "href": "nodes/resonator_spectroscopy_vs_current_node.html#resonator-spectroscopy-as-a-function-of-the-coupler-current",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The qubit spectroscopy as a function of the coupler current is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is conducted by varying the current applied to the coupler and measuring the resonator response at each current value. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the minimum values of the resonator spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the resonator and the coupler.\n\n\nDesctibed in resonator spectroscopy.\n\n\n\nThe ResonatorSpectroscopyVsCurrentNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe ResonatorSpectroscopyVsCurrentCouplerAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe ResonatorSpectroscopyVsCurrentQubitAnalysis performs the analysis of a qubit combining the results of all resonator spectroscopy measurements looking primarely for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler cross the resonator frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region.\nIn cases where the resonator frequency is high and no “U” shape appears—resulting instead in two connected \\(\\bigcap\\) regions—the transition point is inferred from where the two regions meet. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nEach individual resonator spectroscopy is performed by the standard ResonatorSpectroscopyQubitAnalysis class described in the resonator spectroscopy node.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy vs current"
    ]
  },
  {
    "objectID": "nodes/qubit_spectroscopy_vs_current_node.html",
    "href": "nodes/qubit_spectroscopy_vs_current_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The qubit spectroscopy as a function of the coupler current is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe QubitSpectroscopyVsCurrentNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe QubitSpectroscopyVsCurrentCouplerAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitSpectroscopyVsCurrentQubitAnalysis performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the reconator spectroscopy vs current node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyMaxThresholdQubitAnalysis that is stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher.",
    "crumbs": [
      "Home",
      "Node Library",
      "Qubit spectroscopy vs current"
    ]
  },
  {
    "objectID": "nodes/qubit_spectroscopy_vs_current_node.html#qubit-spectroscopy-as-a-function-of-the-coupler-current",
    "href": "nodes/qubit_spectroscopy_vs_current_node.html#qubit-spectroscopy-as-a-function-of-the-coupler-current",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "The qubit spectroscopy as a function of the coupler current is one of the initial steps in characterizing a coupler and preparing for two-qubit gate implementation. This spectroscopy is performed by varying the current applied to the coupler and measuring the qubit response at each current. The result is a two-dimensional map typically showing regions with a “U” or \\(\\bigcap\\) shape when analyzing the maximum values of the qubit spectroscopies. These shapes indicate changes in coupling and are key to identifying crossing points between the qubit and the coupler.\n\n\nDesctibed in qubit spectroscopy.\n\n\n\nThe QubitSpectroscopyVsCurrentNodeAnalysis class handles the analysis of all selected couplers and manages the associated plots. In addition to the preview plot that is displayed at run time, the analysis saves a file for each qubit containing all the resonator spectroscopies for debugging purposes.\nThe QubitSpectroscopyVsCurrentCouplerAnalysis combines the results of the two qubits in a coupler and the saving of the QOI so that the information of the crossing points is saved per qubit per coupler as described in the base class documentation.\nThe QubitSpectroscopyVsCurrentQubitAnalysis performs the analysis of a qubit combining the results of all qubit spectroscopy measurements looking primarily for the crossing points, i.e. the values of the currents at which there is a change of region from U to \\(\\bigcap\\); these are the point at which the coupler crosses the qubit frequency and can be used to study the properties of the coupler. The main function used is find_crossing_currents, which: * Cleans the data by removing isolated zeros (representing missing or invalid data) and outliers. * Scans the data from left to right, comparing consecutive non-zero values to detect transitions based on a predefined threshold. * Groups transitions occurring within a minimum range to mitigate the effect of noise or spurious detections. * Determines the crossing point as the average position within each grouped transition region. * If a crossing is within a small distance from the coupler-resonator crossing (determined in the reconator spectroscopy vs current node), then is excluded. The current thresholds and interval ranges are optimized for a current step size of 50uA.\nThe analysis of each qubit spectroscopy is performed in a dedicated analysis class, the QubitSpectroscopyMaxThresholdQubitAnalysis that is stored in the analysis file of the spectropy folder in the qubit_control folder. This analysis differs from the standard qubit spectroscopy analysis in that it does not attempt to fit the spectrum using a Lorentzian function. This choice is intentional, as the spectral shape near a crossing is often distorted or lacks a clear peak, making fitting unreliable. Instead, the analysis identifies the maximum point in the spectrum and evaluates its significance using a relevance metric defined as (max - mean) / RMS. If the peak is below threshold, zero is returned instead of the maximum value. The current hardcoded threshold (2.7) is rather conservative and was optimized for analysis with poor qubit spectroscopies, i.e. having multiple peaks instead of a clean one. For well calibrated systems this threshold can be raised up to 3.3 or even higher.",
    "crumbs": [
      "Home",
      "Node Library",
      "Qubit spectroscopy vs current"
    ]
  },
  {
    "objectID": "nodes/punchout_node.html",
    "href": "nodes/punchout_node.html",
    "title": "Resonator spectroscopy as a function of the readout amplitude (aka Punchout)",
    "section": "",
    "text": "The resonator spectroscopy measured as a function of the readout amplitude can be used to extract the optimal readout amplitude. This is possible as the response of the resonator should be constant until too much energy is sent to it. Looking at the changes in the resonator frequency, it is possible to estimate the last stable point and set the readout amplitude.\n\n\nThe qubit and the resonator are coupled. This is one of the fundamentals for the way, the qubit is read out. Being coupled means that when the resonator starts to oscillate, the qubit will do so, as well. Now, by design, we do not want the resonator to affect the qubit too much. Otherwise, the readout would affect the qubit state and that would be bad. This is why when the readout resonator begins to oscillate too much, it will not couple to the qubit anymore. Now in the punchout, the idea is to drive the resonator up to the point until it does not couple to the qubit any longer. As long as it is coupled to the qubit, the frequency is slightly higher. This frequency is called the dressed frequency. By adding energy in form of increasing the amplitude, eventually, the resonator no longe couples significantly to the qubit. In the plot, one can see that the frequency will go down a bit. The new frequency of the resonator is now called the bare frequency.\n\n\n\n\n\nThe PunchoutMeasurement class facilitates the creation of schedules for the punchout experiments. It supports parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. - spec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each resonator. - spec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the readout pulses.\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe PunchoutNodeAnalysis class handles all the operations common to all qubits and produce the figures. The standard figure displays the measurements in each qubit with the selected amplitude shown in each subplot.\nAdditional figures are produced for debugging purposes; specifically, a plot per qubit is produced with all the individual resonator spectroscopies to visually inspect the optimal selection of the amplitude. The amplitude selected is highlighted with a red square.\n\n\n\nThis class perform the analysis of the individual qubit. Each resonator spectroscopy is analyzed with the ResonatorSpectroscopyQubitAnalysis class to obtain the fitted resonator frequency. These values are analyzed starting from the smallest amplitude and consecutive resonator frequencies are compared; if the difference between two frequencies is larger than a threshold (currently 20kHz), then the lowest amplitude is selected as the best amplitude.\n\n\n\n\n\n\nExample punchout for five qubits.\n\n\nOn picture above, one can see the plot for the punchout. The plot shows the resonator frequency as a function of the readout amplitude with the z-axis showing the normalized magnitude in each resonator spectroscopy; the normalization is performed due to the very different range in magnitudes in different spectroscopies. Each plot shows the selected readout amplitude in the figure (red X) and the value in the legend. In this example, the resonator frequency in Q1 (the minimum in each vertical slice representing a resonator spectroscopy at a certain readout amplitude) is flat up to the limit of the explored range, so it probably can be pushed harder but the range would need to be extended. In Q2 it is possible to see that the frequency of the resonator starts to lower with when the amplitude reaches 0.035. This behaviour is even more extreme for the resonators in Q3, Q4 and Q5 that need to be readout with lower amplitudes.\n\n\n\nIn cases like Q1, extending the range is probably necessary to capture the best readout amplitude. However, the problem is usually caused by values of the amplitudes that induce a small distortion in the resonators spectroscopy. For this reason, additional figures, one per qubit, are available displaying each resonator spectroscopy as shown below.\n\n\n\nExample resonator spectroscopies for Q5.\n\n\nIn this figure, it is possible to see how the resonator spectroscopy is progressively distorted and how, already in the second row, the fits are not as good as in the plot for the selected amplitude (subplot circled in red).\nEspecially in cases with a small granularity in the amplitude, it may be possible that the selected value is sub-optimal, looking at these plots will help fine-tune the value of the readout amplitude.\n\n\n\n\nPRL 105.100505\nRev. Sci. Instrum. 86, 024706 (2015)",
    "crumbs": [
      "Home",
      "Node Library",
      "Punchout"
    ]
  },
  {
    "objectID": "nodes/punchout_node.html#physical-motivation-for-punchout-node",
    "href": "nodes/punchout_node.html#physical-motivation-for-punchout-node",
    "title": "Resonator spectroscopy as a function of the readout amplitude (aka Punchout)",
    "section": "",
    "text": "The qubit and the resonator are coupled. This is one of the fundamentals for the way, the qubit is read out. Being coupled means that when the resonator starts to oscillate, the qubit will do so, as well. Now, by design, we do not want the resonator to affect the qubit too much. Otherwise, the readout would affect the qubit state and that would be bad. This is why when the readout resonator begins to oscillate too much, it will not couple to the qubit anymore. Now in the punchout, the idea is to drive the resonator up to the point until it does not couple to the qubit any longer. As long as it is coupled to the qubit, the frequency is slightly higher. This frequency is called the dressed frequency. By adding energy in form of increasing the amplitude, eventually, the resonator no longe couples significantly to the qubit. In the plot, one can see that the frequency will go down a bit. The new frequency of the resonator is now called the bare frequency.",
    "crumbs": [
      "Home",
      "Node Library",
      "Punchout"
    ]
  },
  {
    "objectID": "nodes/punchout_node.html#code-structure",
    "href": "nodes/punchout_node.html#code-structure",
    "title": "Resonator spectroscopy as a function of the readout amplitude (aka Punchout)",
    "section": "",
    "text": "The PunchoutMeasurement class facilitates the creation of schedules for the punchout experiments. It supports parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. - spec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each resonator. - spec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the readout pulses.\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe PunchoutNodeAnalysis class handles all the operations common to all qubits and produce the figures. The standard figure displays the measurements in each qubit with the selected amplitude shown in each subplot.\nAdditional figures are produced for debugging purposes; specifically, a plot per qubit is produced with all the individual resonator spectroscopies to visually inspect the optimal selection of the amplitude. The amplitude selected is highlighted with a red square.\n\n\n\nThis class perform the analysis of the individual qubit. Each resonator spectroscopy is analyzed with the ResonatorSpectroscopyQubitAnalysis class to obtain the fitted resonator frequency. These values are analyzed starting from the smallest amplitude and consecutive resonator frequencies are compared; if the difference between two frequencies is larger than a threshold (currently 20kHz), then the lowest amplitude is selected as the best amplitude.\n\n\n\n\n\n\nExample punchout for five qubits.\n\n\nOn picture above, one can see the plot for the punchout. The plot shows the resonator frequency as a function of the readout amplitude with the z-axis showing the normalized magnitude in each resonator spectroscopy; the normalization is performed due to the very different range in magnitudes in different spectroscopies. Each plot shows the selected readout amplitude in the figure (red X) and the value in the legend. In this example, the resonator frequency in Q1 (the minimum in each vertical slice representing a resonator spectroscopy at a certain readout amplitude) is flat up to the limit of the explored range, so it probably can be pushed harder but the range would need to be extended. In Q2 it is possible to see that the frequency of the resonator starts to lower with when the amplitude reaches 0.035. This behaviour is even more extreme for the resonators in Q3, Q4 and Q5 that need to be readout with lower amplitudes.\n\n\n\nIn cases like Q1, extending the range is probably necessary to capture the best readout amplitude. However, the problem is usually caused by values of the amplitudes that induce a small distortion in the resonators spectroscopy. For this reason, additional figures, one per qubit, are available displaying each resonator spectroscopy as shown below.\n\n\n\nExample resonator spectroscopies for Q5.\n\n\nIn this figure, it is possible to see how the resonator spectroscopy is progressively distorted and how, already in the second row, the fits are not as good as in the plot for the selected amplitude (subplot circled in red).\nEspecially in cases with a small granularity in the amplitude, it may be possible that the selected value is sub-optimal, looking at these plots will help fine-tune the value of the readout amplitude.\n\n\n\n\nPRL 105.100505\nRev. Sci. Instrum. 86, 024706 (2015)",
    "crumbs": [
      "Home",
      "Node Library",
      "Punchout"
    ]
  }
]