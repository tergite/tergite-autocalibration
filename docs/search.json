[
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Troubleshooting\nSome errors occur on more often than others. If you are experiencing an error for which you find a simple hack that you want to share, please add it to this troubleshooting guide.\nAs soon as the description for the hack is getting too long, we are going to add either a proper documentation page about the topic or we just implement it as a feature.\n\nRedis\nOn a Linux machine, if REDIS does not accept a connection, you can do this:\nredis-server --port {your redis port} --daemonize yes\nThe --daenonize yes parameter will enable the machine to run REDIS always in the background. If you do not use that parameter, the REDIS instance will stop as soon as you close the terminal where you started it.\nIf you are getting an error saying:\n15350:M 10 Dec 2024 23:24:17.172 # Warning: Could not create server TCP listening socket *:6379: bind: Address already in use\n15350:M 10 Dec 2024 23:24:17.172 # Failed listening on port 6379 (tcp), aborting.\nThen there might be already running a REDIS instance on that port.\nYou can verify that by typing:\nredis-cli -p {the redis port you had the error with}\nIf it opens the REDIS CLI, then you have an instance already running.\nTo check whether there are already keys in the REDIS server - assuming you have opened the REDIS CLI - you can type:\nkeys *\nThis will give you all the keys that are stored in the REDIS instance. If you are on a shared machine, you should always double-check whether no one else is using the same REDIS instance. Otherwise, it could easily happen that you overwrite someone else’s memory and data will be lost.\nOn Windows REDIS is run on WSL. Kill the session and restart it and run REDIS (if you have not set it to run automatically). Sometimes you may get in the situation that WSL will not open at all, in this case you need to restart the service called “Hyper-V Host Compute Service”\n\n\nSPI\nYou can get an error saying that the default path was not correct or that you get denied permission. Likely there was a connection implemented before and now when trying to start a new one it hangs itself up and refuses communication, you can force this communication channel by running\nsudo chmod 666 /dev/ttyACM0",
    "crumbs": [
      "Home",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html",
    "href": "developer-guide/new_node_creation.html",
    "title": "Creating a new node",
    "section": "",
    "text": "This tutorial is about how to create a new node class. The word node is an overloaded term, used in many contexts, so, what does a node mean here? If you put all steps to characterize a qubit in a chain, then it could be seen as a directed graph with the calibration steps as nodes. Take a look at the node overview to get an impression on how it looks like. In this guide, we will cover the topics:\n\nFile locations for nodes within the framework\nHow to implement a node\nRegister the node in the framework\n\nFollowing these steps, you can easily contribute to the automatic calibration with your own nodes in a couple of hours.\n\n\nThe nodes are located in tergite_autocalibration/lib/nodes. If you open that module, you will find that there are four submodules for different kind of nodes:\n\ncharacterization: For nodes such as the T1, T2 or the randomized benchmarking.\ncoupler: For all nodes that are related to a two-qubit setup with a coupler.\nqubit_control: All nodes that calibrate a quantity of interest for the qubit.\nreadout: All nodes that calibrate a quantity of interest for the resonator.\n\nPlease create a new submodule for your node in one of the four submodules listed above. A sort of template on how a node module should be structured can be found in tergite_autocalibration/lib/base. Essentially, a proper package in the end should contain:\n\n__init__.py: This is an empty file to mark that the folder is a package. Please add this file, because otherwise your classes cannot be found.\nnode.py: A file where the definition of the node class goes.\nanalysis.py: A file where the analysis object is defined.\nmeasurement.py: Contains the measurement object with the schedule.\ntests: A folder with all test function and test data. Read more about unit tests to find out on how to structure them.\nutils: A folder in case you have defined very specific helper classes.\n\nBefore we are going to a look on how this would be implemented in detail, a quick note on naming conventions.\n\n\nSince we are creating a lot of node, measurement and analysis objects, there are some naming conventions to make it more standardized and understandable to learn the framework. Taking the rabi oscillations as an example, we have:\n\nrabi_oscillations: This is the node name, which is used in the commandline interface or for other purposes to pass the node name as a string.\nRabiOscillations: This is the name of the node class.\nRabiOscillationsAnalysis: The name of the respective analysis class.\nRabiOscillationsMeasurement: And the name of the respective measurement class.\n\nWhen there are some more complicated nodes for which you do not know how to name it, please just take a look at the already existing nodes and make a guess how it feels to name it correctly. Also, when there is a node that starts with an abbreviation, please have all letter for the abbreviation capitalized e.g.: cz_calibration would be the node name for the CZCalibration node class.\n\n\n\n\nAll node classes are supposed follow the same interface as described in the BaseNodeclass. Below, for example, you have the rabi oscillations node:\nclass RabiOscillations(ScheduleNode):\n   measurement_obj = RabiOscillationsMeasurement\n   analysis_obj = RabiOscillationsAnalysis\n   qubit_qois = [\"rxy:amp180\"]\n\n   def __init__(self, name: str, all_qubits: list[str], **schedule_keywords):\n      super().__init__(name, all_qubits, **schedule_keywords)\n        self.schedule_samplespace = {\n           \"mw_amplitudes\": {\n              qubit: np.linspace(0.002, 0.90, 61) for qubit in self.all_qubits\n            }\n        }\nAs you can see, it inherits from the ScheduleNode class, which contains a very simple definition of a node that runs a simple sweep over a single quantity. More information about other node classes can be found below. Furthermore, you can see that the node has three class attributes:\n\nmeasurement_obj: Contains the class of the measurement, that defines the pulse schedule for the instruments\nanalysis_obj: Contains the class for the analysis to post-process the measurement results. For example, this could be a simple fitting function.\nqubit_qois: The quantity of interest (QOI), which is returned by the analysis. In case of the rabi oscillations this is the pulse amplitude.\n\nAlso, you can see in the constructor, there is an attribute called schedule_samplespace. Here, we define in the measurement, what quantity will be swept over.\n\n\nThe measurement_obj is implemented in the measurement.py file of your node submodule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node’s samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is implemented in the analysis.py file from your module and contains the class that perform the analysis.\n\n\n\nIn the example above, the node inherits from the class ScheduleNode. This is one option for the node behaviour:\n\nScheduleNode: A node with a simple sweep over the samplespace. The quantify schedule is only compiled once and the parameter values from the samplespace are the input for the schedule function of the measurement.\nExternalParameterNode: A node with a more complex measurement procedure. It allows to run multiple steps sequentially, where each step might require recompilation of the schedule. There is an external parameter involved, which is the part of the generating function within the schedule.\n\nWhen you are implementing a node, you can choose which of the two abstract node classes fit better with the behaviour of your new node. Also, if you want to implement a more sophisticated measurement procedure, you can override the procedures in the\n\n\n\n\nTo add the node to the framework, you have to register it in two places - the node factory and the calibration graph. Also, please do not forget to write documentation for your node.\n\n\nA factory is a programming pattern to create complex objects such as our nodes in a bit more clearly interfaced way. The factory contains a register for all nodes, that map their name to the respective class where the node is implemented. When you are adding a node, please register your node name, by adding it to the tergite_autocalibration.lib.utils.node_factory.NodeFactory class under the self.node_name_mapping attribute in the dictionary.\n\n\n\nIn the file tergite_autocalibration/lib/utils/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\n\n\nPlease add your node to the list of available nodes in this documentation. If possible, please create a separate page that explains what your node is doing and link it there.\nAdd any relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#where-are-the-nodes-located",
    "href": "developer-guide/new_node_creation.html#where-are-the-nodes-located",
    "title": "Creating a new node",
    "section": "",
    "text": "The nodes are located in tergite_autocalibration/lib/nodes. If you open that module, you will find that there are four submodules for different kind of nodes:\n\ncharacterization: For nodes such as the T1, T2 or the randomized benchmarking.\ncoupler: For all nodes that are related to a two-qubit setup with a coupler.\nqubit_control: All nodes that calibrate a quantity of interest for the qubit.\nreadout: All nodes that calibrate a quantity of interest for the resonator.\n\nPlease create a new submodule for your node in one of the four submodules listed above. A sort of template on how a node module should be structured can be found in tergite_autocalibration/lib/base. Essentially, a proper package in the end should contain:\n\n__init__.py: This is an empty file to mark that the folder is a package. Please add this file, because otherwise your classes cannot be found.\nnode.py: A file where the definition of the node class goes.\nanalysis.py: A file where the analysis object is defined.\nmeasurement.py: Contains the measurement object with the schedule.\ntests: A folder with all test function and test data. Read more about unit tests to find out on how to structure them.\nutils: A folder in case you have defined very specific helper classes.\n\nBefore we are going to a look on how this would be implemented in detail, a quick note on naming conventions.\n\n\nSince we are creating a lot of node, measurement and analysis objects, there are some naming conventions to make it more standardized and understandable to learn the framework. Taking the rabi oscillations as an example, we have:\n\nrabi_oscillations: This is the node name, which is used in the commandline interface or for other purposes to pass the node name as a string.\nRabiOscillations: This is the name of the node class.\nRabiOscillationsAnalysis: The name of the respective analysis class.\nRabiOscillationsMeasurement: And the name of the respective measurement class.\n\nWhen there are some more complicated nodes for which you do not know how to name it, please just take a look at the already existing nodes and make a guess how it feels to name it correctly. Also, when there is a node that starts with an abbreviation, please have all letter for the abbreviation capitalized e.g.: cz_calibration would be the node name for the CZCalibration node class.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#node-implementation-details",
    "href": "developer-guide/new_node_creation.html#node-implementation-details",
    "title": "Creating a new node",
    "section": "",
    "text": "All node classes are supposed follow the same interface as described in the BaseNodeclass. Below, for example, you have the rabi oscillations node:\nclass RabiOscillations(ScheduleNode):\n   measurement_obj = RabiOscillationsMeasurement\n   analysis_obj = RabiOscillationsAnalysis\n   qubit_qois = [\"rxy:amp180\"]\n\n   def __init__(self, name: str, all_qubits: list[str], **schedule_keywords):\n      super().__init__(name, all_qubits, **schedule_keywords)\n        self.schedule_samplespace = {\n           \"mw_amplitudes\": {\n              qubit: np.linspace(0.002, 0.90, 61) for qubit in self.all_qubits\n            }\n        }\nAs you can see, it inherits from the ScheduleNode class, which contains a very simple definition of a node that runs a simple sweep over a single quantity. More information about other node classes can be found below. Furthermore, you can see that the node has three class attributes:\n\nmeasurement_obj: Contains the class of the measurement, that defines the pulse schedule for the instruments\nanalysis_obj: Contains the class for the analysis to post-process the measurement results. For example, this could be a simple fitting function.\nqubit_qois: The quantity of interest (QOI), which is returned by the analysis. In case of the rabi oscillations this is the pulse amplitude.\n\nAlso, you can see in the constructor, there is an attribute called schedule_samplespace. Here, we define in the measurement, what quantity will be swept over.\n\n\nThe measurement_obj is implemented in the measurement.py file of your node submodule. To initialize we require a dictionary of the extended transmons:\ntransmons: dict[str, ExtendedTransmon]\nIt must contain a method called schedule_function that expects the node’s samplespace as input and returns the complete schedule.\n\n\n\nThe analysis_obj is implemented in the analysis.py file from your module and contains the class that perform the analysis.\n\n\n\nIn the example above, the node inherits from the class ScheduleNode. This is one option for the node behaviour:\n\nScheduleNode: A node with a simple sweep over the samplespace. The quantify schedule is only compiled once and the parameter values from the samplespace are the input for the schedule function of the measurement.\nExternalParameterNode: A node with a more complex measurement procedure. It allows to run multiple steps sequentially, where each step might require recompilation of the schedule. There is an external parameter involved, which is the part of the generating function within the schedule.\n\nWhen you are implementing a node, you can choose which of the two abstract node classes fit better with the behaviour of your new node. Also, if you want to implement a more sophisticated measurement procedure, you can override the procedures in the",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/new_node_creation.html#register-the-node-in-the-framework",
    "href": "developer-guide/new_node_creation.html#register-the-node-in-the-framework",
    "title": "Creating a new node",
    "section": "",
    "text": "To add the node to the framework, you have to register it in two places - the node factory and the calibration graph. Also, please do not forget to write documentation for your node.\n\n\nA factory is a programming pattern to create complex objects such as our nodes in a bit more clearly interfaced way. The factory contains a register for all nodes, that map their name to the respective class where the node is implemented. When you are adding a node, please register your node name, by adding it to the tergite_autocalibration.lib.utils.node_factory.NodeFactory class under the self.node_name_mapping attribute in the dictionary.\n\n\n\nIn the file tergite_autocalibration/lib/utils/graph.py in the list graph_dependencies insert the edges that describe the position of the new node in the Directed Acyclic Graph. There are two entries required (or one entry if the new node is the last on its path):\n\n('previous_node','new_node')\n('new_node', 'next_node')\n\n\n\n\nPlease add your node to the list of available nodes in this documentation. If possible, please create a separate page that explains what your node is doing and link it there.\nAdd any relevant information on how to use your node, dependencies and reference to publication as needed for allowing other to use the code you developed.\nDetails on the implementation on the Node types section.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Create a new node"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html",
    "href": "developer-guide/writing_documentation.html",
    "title": "Writing documentation",
    "section": "",
    "text": "Writing good documentation is very important for new people to understand how to use things. Often, we do not have time to write good guides, because we are having deadlines or other things to do that are more interesting and less boring than writing down what we already seem to know so well.\nThis guide is a small guide on how to write a good guide. It does not aim to provide an explanation on how to write the perfect documentation, because these resources already exist on the internet. However, it should make at least curious on how to write documentation with some short general rules:\n\nThink like the reader\nGo step by step\nBe precise\nFollow standards\nDiscuss\n\nIn the second part of the tutorial, we will focus on how to write documentation especially for this project.\nSo, scroll down if you want to start reading there.\n\n\nWhen you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.\n\n\n\n\n\nIn this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.\n\n\n\n\nWhen you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "href": "developer-guide/writing_documentation.html#example-on-writing-documentation-in-general",
    "title": "Writing documentation",
    "section": "",
    "text": "When you explain how things work, it is often better to take it step by step. Also, do not hesitate to make small steps. Tiny ones.\nKeep in mind to think like the reader:\n\nWhat is the previous knowledge about a topic\n\nMost of the people reading our docs are probably students, so maybe they are not familiar with all git commands and Linux tricks.\n\nProvide a structure/agenda for your guide\n\nLet us say you want to write a guide about the installation of Miniconda, which contains a lot of steps and the installation process looks like this example.\n\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\n\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\n\nconda init bash\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\n\nconda create -n new-environment python=3.9\nconda activate new-environment\n\ncd ~\nmkdir repos\ncd repos\ngit clone git@my-repo-url\ncd my-repo\npip install -e .\nHow can this guide for an installation look better? Well, while writing think of yourself explaining the same installation procedure to a colleague. You would probably tell some of the commands, but in between you would also briefly discuss what they are doing. Maybe you are experiencing an error at some point. Maybe you want to give some more background knowledge in case the reader does not have it. In the case of our team it very important to give these kind of hints, because we are working in an interdisciplinary environment and followed different educational paths.\nNow, let’s see how the installation could look like. Let us assume we already wrote an introduction, the prerequisites are clear, and we are just explaining the installation itself.\n\n\n\nNow this is the better version, because it breaks down the process into:\n\nDownloading the installation file\nInstalling conda\nTroubleshooting some common problems that can happen during the installation\nShowing what would be the next step after the installation\n\ncd ~\nmkdir tmp\ncd tmp\nwget SomeInstallationFileForMiniconda\nFirst, we are navigating to our home directory and create a temporary directory where we store the Miniconda installation file. With the wget command, we download the file. Note that there are different versions of the installer, depending on which operating system you use. A full list of installers can be found on the Miniconda webpage . The reason why we are using Miniconda and not Anaconda is because it takes less disk space. As soon as we have downloaded the installer file, we can continue with the installation.\nchmod u+x SomeInstallationFileForMiniconda\n./SomeInstallationFileForMiniconda\nWe have to make the installer executable and run it. During the installation process itself, just follow the standard recommendations and paths that the installer selects during the installation. After the installation it can be possible that you have to do some tiny adjustments. With\nconda init bash\nyou can modify your shell to show the conda path. And with\nexport PATH=/home/&lt;YOUR\\_USERNAME&gt;/miniconda3/bin:$PATH\nyou are adding conda to the PATH variable. That should be done automatically during the installation, but sometimes it does not work properly. You can verify the installation by typing:\nconda --version\nAfter the installation is complete, we can create our conda environment.\nconda create -n new-environment python=3.9\nconda activate new-environment\nNote that we are using Python 3.9 in our environment. This is because we are having some dependencies with another library, which will be explained in more detail in the remarks section. If you are running the installation on macOS, please find additional resources in the respective guide how to use conda environments on macOS. We can now start to clone our repository and install the dependencies for the project.\ncd ~\ncd repos\ngit clone git@my-repo-url\nIf you have not done yet, you can create the repos directory using mkdir repos. Please read this other guide that explains how to use Git. After you cloned the repository, we can navigate in there and install our dependencies.\ncd my-repo\npip install -e .\nThis can take up to three minutes. If you are running into any problems during the installation please contact one of the other team members and verify whether it works for them or try to find a fix if you know how to approach it. If you have fixed errors that happened during the installation, please put a note into this installation guide, so, other people with the same issue have an easier time to solve it.\nWhat is now better in this second example are the explanations for every step. For your brain it is way easier to come back to one of these steps, and you learn way more about what you are doing when there are small explanations than when just copy and pasting console commands.\n\nTry to group commands or tiny steps that belong together, otherwise it looks a bit scattered. But also do not have to many loose blocks flying around.\nDo not write too much, but take some time to explain some of the backgrounds even if you think it is clear. If there is a lot of background, you can also link to some page in the internet which already has done the work and provides a tutorial. Finally, make sure that you are writing in a consistent style and provide enough examples.\nWhenever your guide is finished, share it with others and discuss. It is probably not perfect (yet) and other people might have valuable feedback from their own experiences with the problem you are describing.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "href": "developer-guide/writing_documentation.html#writing-documentation-for-the-automatic-calibration",
    "title": "Writing documentation",
    "section": "",
    "text": "In this section, we will go through the specific processes that are important when writing documentation:\n\nInstalling quarto\nStructure of the files\nAdding a navigation entry\nAdvanced features of quarto\n\n\n\nIn the tergite-autocalibration repository we are using Quarto to render documentation. Quarto is very versatile and can - among other document types - render markdown and Jupyter notebooks. If you have been following the steps in the developer guide introduction, you should have quarto already installed. Otherwise, you can do it by running:\npip install quarto\nTo render a simple preview of your documentation, please open a terminal inside the documentation folder and run:\ncd documentation\nquarto preview\nThis will open a browser window with the rendered quarto documentation pages.\n\n\n\nMaybe you noticed that on the top-level of the repository there are two folders, one called docs and another one called documentation. This is because they have two different purposes:\n\ndocumentation: Contains the markdown files and Jupyter notebooks to create the documentation from. These are the files that you edit.\ndocs: Is the output HTML after running quarto render, which is displayed on the website. You do not edit these files. They will always be generated from the files in the documentation folder.\n\nNow, let us have a look at the documentation folder, because this is the one we are working with the most. It is structured:\n\n.assets: There you put images and style/formatting material.\n.quarto: Do not touch this folder and do not commit it to git, because it contains temporary files during the rendering process.\ndeveloper-guide, nodes and user-guide: Contains the respective content for the pages.\nThen there are a couple of pages from the top-level of the documentation.\nAnd a file called _quarto.yml. This file is important, because it defines how things are rendered.\n\n\n\n\nIn here, the most relevant to be touched during adding documentation is the sidebar section. Imagine you are adding a new page e.g. about a calibration node, and you want to add it to the navigation. Then, you would add an entry at the correct position in the _quarto.yml file for the sidebar.\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Node Library\"\n        contents:\n          - text: \"Overview\"\n            href: available_nodes.qmd\n          - text: \"Resonator spectroscopy\"\n            href: nodes/resonator_spectroscopy_node.qmd\n          - text: \"My new node\"\n            href: nodes/my_new_node.qmd\nIt is pretty self-explaining where to put the node when you see the rendered version in your browser.\n\n\n\nAs you noticed, quarto does not render from normal .md markdown files, but from .qmd quarto markdown files. These are extending the markdown functionality with some special features. Here, we will show them along with some normal useful feature from markdown.\nCode highlighting\nImagine you want to have a block to show code. What you write inside your markdown file would be:\n```python\nvariable = 123\nprint(\"Hello world\")\n```\nAnd the output would look like:\nvariable = 123\nprint(\"Hello world\")\nGraphs\nFor the calibration nodes, we are using a graph to chain them. This graph is rendered with mermaid.\n```{mermaid}\ngraph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n```\nWith the style attribute, you can define the colour of the node. With the click attribute, add a link on a node inside the graph.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "developer-guide/writing_documentation.html#next-steps",
    "href": "developer-guide/writing_documentation.html#next-steps",
    "title": "Writing documentation",
    "section": "",
    "text": "When you reached the point that you are already writing the perfect documentation, you are probably also done reading the documentation. So, no next steps to read up upon. You can now write even more documentation or just code and explore quantum physics :)",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Writing documentation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Documentation of the Tergite Automatic Calibration",
    "section": "",
    "text": "Documentation of the Tergite Automatic Calibration\nThe tergite-autocalibration is a toolkit to ease calibrating quantum devices for superconducting platforms. The project contains an orchestration manager, a collection of calibration schedules and a collection of post-processing and analysis routines. It is tailored to tune-up the 25 qubits QPU at Chalmers, which is receiving generous funding by the Wallenberg Centre for Quantum Technology (WACQT) for research, development and operation.\n\nUser Guide\nA tutorial on how to get started with the automatic calibration. This tutorial contains a guide on how to use the commandline interface with quick commands. Further, there is an introduction into configuration files.\n\nGetting started\nOperation\nConfiguration Files\n\n\n\nNode Library\nThe main principle behind the automatic calibration is based on calibrating nodes in a graph structure. A node contains all the measurement and analysis classes to find the quantity of interest - for qubits and couplers. If you are interested in implementing a new node, it might be worth it to check whether there are existing nodes that you can use to find the qubit properties you are looking to calibrate.\n\n\nDeveloper Guide\nThis repository is an actively developed open-source project and also part of the Tergite full-stack software ecosystem to operate a quantum computer. Hence, you are more than welcome to contribute to the project with your own ideas that fit into the framework. To familiarize yourself with the existing classes and the architecture of the automatic calibration, please take a look into the development guide. Here, you can also find best practices and information about our design philosophy.\n\nCreate node classes\nWrite unit tests\nDocumentation guidelines",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "available_nodes.html",
    "href": "available_nodes.html",
    "title": "Node Library",
    "section": "",
    "text": "graph TD\n    A[Resonator Spectroscopy] --&gt; B(Qubit Spectroscopy)\n    B --&gt; C[Rabi Oscillations]\n    C --&gt; D[Ramsey Correction]\n    D --&gt; E[Motzoi Parameter]\n    E --&gt; F[Resonator Spectroscopy 1]\n    F --&gt; C1[T1] --&gt; C2[T2] --&gt; C3[Randomized Benchmarking]\n    F --&gt; F1(Qubit 12 Spectroscopy) --&gt; G(Rabi 12 Oscillations)\n    F --&gt; H1(2 States Discrimination)\n    G --&gt; H2(3 States Discrimination)\n        \n    click A href \"nodes/resonator_spectroscopy_node.html\"\n    click B href \"nodes/qubit_spectroscopy_node.html\"\n\n    style A fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style B fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style D fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style E fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style F fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style C1 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C2 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style C3 fill:#ffffcc,stroke:#333,stroke-width:2px\n    style F1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style G fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H1 fill:#ffe6cc,stroke:#333,stroke-width:2px\n    style H2 fill:#ffe6cc,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\npunchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization\n\n\n\n\n\nqubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination\n\n\n\n\n\ncoupler_spectroscopy\ncoupler_resonator_spectroscopy\n\n\n\n\n\nT1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#readout-nodes",
    "href": "available_nodes.html#readout-nodes",
    "title": "Node Library",
    "section": "",
    "text": "punchout\nresonator_spectroscopy\nresonator_spectroscopy_1\nresonator_spectroscopy_2\nro_frequency_two_state_optimization\nro_frequency_three_state_optimization\nro_amplitude_two_state_optimization\nro_amplitude_three_state_optimization",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#qubit-control-nodes",
    "href": "available_nodes.html#qubit-control-nodes",
    "title": "Node Library",
    "section": "",
    "text": "qubit_01_spectroscopy\nqubit_01_spectroscopy_pulsed\nrabi_oscillations\nramsey_correction\nqubit_12_spectroscopy_pulsed\nqubit_12_spectroscopy_multidim\nrabi_oscillations_12\nramsey_correction_12\nadaptive_motzoi_parameter\nn_rabi_oscillations\nstate_discrimination",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#coupler-nodes",
    "href": "available_nodes.html#coupler-nodes",
    "title": "Node Library",
    "section": "",
    "text": "coupler_spectroscopy\ncoupler_resonator_spectroscopy",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "available_nodes.html#characterization-nodes",
    "href": "available_nodes.html#characterization-nodes",
    "title": "Node Library",
    "section": "",
    "text": "T1\nT2\nT2_echo\nrandomized_benchmarking\nall_XY",
    "crumbs": [
      "Home",
      "Node Library",
      "Overview"
    ]
  },
  {
    "objectID": "nodes/resonator_spectroscopy_node.html",
    "href": "nodes/resonator_spectroscopy_node.html",
    "title": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis",
    "section": "",
    "text": "Resonator Spectroscopy in Qubit-Resonator Interaction Analysis\nResonator spectroscopy is a powerful tool for studying the interaction between qubits and resonators. By probing the resonator’s response across various frequencies, we can extract critical parameters, such as the resonant frequency and Q-factor, which are essential for optimizing qubit-resonator interactions.\n\nClass: Resonator_Spectroscopy\nThe Resonator_Spectroscopy class is designed to conduct resonator spectroscopy for transmon qubits. It takes as input a dictionary of transmon qubits and their respective states.\n\nMethod: schedule\nThe schedule method generates a schedule to perform resonator spectroscopy. This process includes:\n\nClock Initialization: Initializes the clocks for each qubit based on the specified qubit state.\nQubit Reset: Resets the qubit to a known state.\nFrequency Probing: Applies a square pulse at various frequencies.\nSignal Measurement: Measures the response signal, capturing data on the resonator’s behavior.\n\n\n\n\nClass: ResonatorSpectroscopyQubitAnalysis\nThe ResonatorSpectroscopyQubitAnalysis class is used for analyzing resonator spectroscopy data, enabling the extraction of the resonant frequency and Q-factor. This class takes as parameters:\n\nqubit_name: A string representing the qubit under measurement.\nredis_fields: The directory for data storage.\n\n\nMethod: analyse_qubit\nThe analyse_qubit method processes and fits the data to determine the resonator’s resonant frequency and loaded Q-factor. The analyse_qubitfit should resemble a negative Gaussian distribution.\n\n\n\nOutput: xarray.Dataset\nThe dataset returned by this analysis is an xarray.Dataset, which includes:\n\nFrequency Sweep Data: The set of frequencies used during the sweep.\nTransmission Response: The measured response of the resonator at each frequency.\n\nThis dataset provides essential insights into the resonator’s properties, allowing for precise tuning of qubit-resonator interactions.",
    "crumbs": [
      "Home",
      "Node Library",
      "Resonator spectroscopy"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html",
    "href": "user-guide/configuration_files.html",
    "title": "Configuration",
    "section": "",
    "text": "To run the autocalibration you will need to configure it with a couple of different configuration files. Doing the configuration with a pre-built configuration package will take just some minutes. On an unknown experimental device without pre-built files, it probably takes slightly longer.\nAfter reading this guide, you will know:\n\nHow to set basic environment variables\nWhat is a configuration package and how is it structured\n\nTake a look into the operation manual to see how to load and save configuration packages.\n\n\nComputer systems generally use variables on a system level e.g. to store global variables. These variables usually have a name with UPPER_CASE_LETTERS. Also, there is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown. Most of the values have reasonable defaults, but of course the CLUSTER_IP is required when you measure on a cluster and do not want to run only on dummy data.\n\n\n\nFor all other configuration, there is a so-called configuration package. The reason to have a configuration package is to have all configuration files in one place.\nThis is how an example configuration package looks like:\n\nconfigs/: A folder with the configuration files\n\ncluster_config.json: The configuration for the cluster\ndevice_config.toml: The configuration with values related to the device/chip\nnode_config.toml: Some device configuration that is overwritten when a certain node is executed\nrun_config.toml: Run-specific parameters such as the qubits and the target node to calibrate\nspi_config.toml: Defines the spi and groups of couplers\nuser_samplespace.py: Define custom sweeps for the nodes\n\nadditional_files/: A folder with other additional files\n\nmixer_calibration.csv: E.g. the mixer calibration values\n\nwiring_diagrams/: A folder with even more additional files\n\nwiring_diagram.png: E.g. a wiring diagram\n\nconfiguration.meta.toml: The configuration file that describes the structure of the configuration package\n\nThe templates for a full configuration package can be found in tergite_autocalibration/config/templates. There is a .default template to illustrate the general structure of a configuration package. Furthermore, there are some pre-build configuration packages for different kind of setups.\nFor a configuration to be detected by the application, the configuration.meta.toml file should be placed in the root folder of the tergite-autocalibration repository. All filepaths relative to the configuration files have to be able to be resolved. Now, we will go through all the details of these configuration files.\n\n\nThis is the file that always has to be part of the configuration package. It tells the machine where all other configuration files are located, which is crucial to make the automatic loading and saving work. A very simple version of the configuration.meta.toml belonging to the screenshot above would look like this:\npath_prefix = 'configs'\n\n[files]\ncluster_config = 'cluster_config.json'\ndevice_config = 'device_config.toml'\nnode_config = 'node_config.toml'\nrun_config = 'run_config.toml'\nspi_config = 'spi_config.toml'\nuser_samplespace = 'user_samplespace.py'\n\n[misc]\nmixer_calibrations = \"additional_files\"\nwiring_diagrams = \"wiring_diagrams\"\nThe main sections in that .toml file are:\n\npath_prefix: This refers to the folder name into which you would put the other configuration files. If you leaved it empty, this would mean that all configuration files would be inside the same folder with the configuration.meta.toml file.\nThe files section: Here, you put the paths to the configuration files. It can be one or more of the six above. For example, you could also just define cluster_config and device_config and it would be still a valid configuration package. However, maybe during runtime, it would break the code. E.g. if you run without a cluster configuration, it could work fine if you are running a dummy measurement without real hardware, but if you want to measure on real hardware, you would need the cluster configuration. More about the configuration files is described in the sections below for each of the files individually.\nThe misc section: You can add as many folder as you want to that section. Here, we are adding one more folder to the configuration package with additional files. This section is meant to add files like mixer corrections or a wiring diagram, which do not follow a well-defined standard, but might be useful information to be transferred with the configuration package.\n\nSince the configuration.meta.toml file always should reflect how the configuration package looks like, please update it as soon as you add or delete any configuration files from your package.\nNow, in the folder, there are these six configuration files:\n\nCluster configuration\nDevice configuration\nNode configuration\nRun configuration\nSPI configuration (optional, only required for two-qubit calibration)\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nIn the following, there are some more detailed descriptions of what these files mean and contain. More information can also be found in the templates and example configuration files.\n\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Part of a cluster definition\n{\n  \"config_type\": \"quantify_scheduler.backends.qblox_backend.QbloxHardwareCompilationConfig\",\n  \"hardware_description\": {\n    \"clusterA\": {\n      \"instrument_type\": \"Cluster\",\n      \"ref\": \"internal\",\n      \"modules\": {\n        \"2\": {\n          \"instrument_type\": \"QCM_RF\"\n        },\n        \"10\": {\n          \"instrument_type\": \"QRM_RF\"\n        }\n      }\n    }\n  },\n  \"hardware_options\": {\n    \"modulation_frequencies\": {\n      \"q00:mw-q00.01\": {\n        \"lo_freq\": 3946000000.0\n      },\n      ...\n    },\n    \"mixer_corrections\": {\n      \"q00:mw-q00.01\": {\n        \"dc_offset_i\": 0.0,\n        \"dc_offset_q\": 0.0,\n        \"amp_ratio\": 1.0,\n        \"phase_error\": 0.0\n      },\n      ...\n    }\n    },\n  \"connectivity\": {\n    \"graph\": {\n      \"directed\": false,\n      \"multigraph\": false,\n      \"graph\": {},\n      \"nodes\": [\n        {\n          \"instrument_name\": \"clusterA\",\n          \"id\": \"clusterA.module2.complex_output_0\"\n        },\n        ...\n      ],\n      \"links\": [\n        {\n          \"source\": \"clusterA.module2.complex_output_0\",\n          \"target\": \"q00:mw\"\n        },\n        ...\n      ]\n    }\n    }\n}\nThe file in the template package is cluster_configuration.json.\nYou can find more information about the hardware configuration in the documentation of quantify-scheduler\nMigrating old hardware configurations to match quantify-scheduler&gt;=0.18.0\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\n\n[device.qubit.all]\nmeasure.integration_time = 2.5e-6\nmeasure_1.integration_time = 2.5e-6\nmeasure_2.integration_time = 2.5e-6\nrxy.duration = 28e-9\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\n\n\n\nIn this file there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\n\n\n\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the property is a property known to the node. This would load and overwrite the configurations made in the device configuration.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is node_config.toml.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py.\n\n\n\n\nRead about the commandline interface, which contains a chapter about how to load and save configuration packages.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#environment-variables",
    "href": "user-guide/configuration_files.html#environment-variables",
    "title": "Configuration",
    "section": "",
    "text": "Computer systems generally use variables on a system level e.g. to store global variables. These variables usually have a name with UPPER_CASE_LETTERS. Also, there is a convention to store those variables in a .env file on the root-level of your project and load the variables before running a program. The template for the environmental variables of the tergite-autocalibration can be found in the .example.env file on root-level of the repository. E.g. if you have cloned the repository into /home/user/repos/tergite-autocalibration, then your example template should be located there.\nCopy the template and update values according to the instructions. The template file itself provides instructions on how to update the values.\ncp .example.env .env\nValues that can be set in the environment are e.g. PLOTTING and this variable determines whether plots should be shown. Most of the values have reasonable defaults, but of course the CLUSTER_IP is required when you measure on a cluster and do not want to run only on dummy data.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#configuration-packages",
    "href": "user-guide/configuration_files.html#configuration-packages",
    "title": "Configuration",
    "section": "",
    "text": "For all other configuration, there is a so-called configuration package. The reason to have a configuration package is to have all configuration files in one place.\nThis is how an example configuration package looks like:\n\nconfigs/: A folder with the configuration files\n\ncluster_config.json: The configuration for the cluster\ndevice_config.toml: The configuration with values related to the device/chip\nnode_config.toml: Some device configuration that is overwritten when a certain node is executed\nrun_config.toml: Run-specific parameters such as the qubits and the target node to calibrate\nspi_config.toml: Defines the spi and groups of couplers\nuser_samplespace.py: Define custom sweeps for the nodes\n\nadditional_files/: A folder with other additional files\n\nmixer_calibration.csv: E.g. the mixer calibration values\n\nwiring_diagrams/: A folder with even more additional files\n\nwiring_diagram.png: E.g. a wiring diagram\n\nconfiguration.meta.toml: The configuration file that describes the structure of the configuration package\n\nThe templates for a full configuration package can be found in tergite_autocalibration/config/templates. There is a .default template to illustrate the general structure of a configuration package. Furthermore, there are some pre-build configuration packages for different kind of setups.\nFor a configuration to be detected by the application, the configuration.meta.toml file should be placed in the root folder of the tergite-autocalibration repository. All filepaths relative to the configuration files have to be able to be resolved. Now, we will go through all the details of these configuration files.\n\n\nThis is the file that always has to be part of the configuration package. It tells the machine where all other configuration files are located, which is crucial to make the automatic loading and saving work. A very simple version of the configuration.meta.toml belonging to the screenshot above would look like this:\npath_prefix = 'configs'\n\n[files]\ncluster_config = 'cluster_config.json'\ndevice_config = 'device_config.toml'\nnode_config = 'node_config.toml'\nrun_config = 'run_config.toml'\nspi_config = 'spi_config.toml'\nuser_samplespace = 'user_samplespace.py'\n\n[misc]\nmixer_calibrations = \"additional_files\"\nwiring_diagrams = \"wiring_diagrams\"\nThe main sections in that .toml file are:\n\npath_prefix: This refers to the folder name into which you would put the other configuration files. If you leaved it empty, this would mean that all configuration files would be inside the same folder with the configuration.meta.toml file.\nThe files section: Here, you put the paths to the configuration files. It can be one or more of the six above. For example, you could also just define cluster_config and device_config and it would be still a valid configuration package. However, maybe during runtime, it would break the code. E.g. if you run without a cluster configuration, it could work fine if you are running a dummy measurement without real hardware, but if you want to measure on real hardware, you would need the cluster configuration. More about the configuration files is described in the sections below for each of the files individually.\nThe misc section: You can add as many folder as you want to that section. Here, we are adding one more folder to the configuration package with additional files. This section is meant to add files like mixer corrections or a wiring diagram, which do not follow a well-defined standard, but might be useful information to be transferred with the configuration package.\n\nSince the configuration.meta.toml file always should reflect how the configuration package looks like, please update it as soon as you add or delete any configuration files from your package.\nNow, in the folder, there are these six configuration files:\n\nCluster configuration\nDevice configuration\nNode configuration\nRun configuration\nSPI configuration (optional, only required for two-qubit calibration)\nCustom user samplespace configuration (optional, only required if you are sweeping on a very specific range of parameters)\n\nIn the following, there are some more detailed descriptions of what these files mean and contain. More information can also be found in the templates and example configuration files.\n\n\n\nA QBLOX cluster consists of a couple of modules of which each can have multiple input/output options for SMI cables. In the cluster configuration the connection is made between these QBLOX cluster physical ports and clocks to the qubits and couplers of the QPU.\nExample: Part of a cluster definition\n{\n  \"config_type\": \"quantify_scheduler.backends.qblox_backend.QbloxHardwareCompilationConfig\",\n  \"hardware_description\": {\n    \"clusterA\": {\n      \"instrument_type\": \"Cluster\",\n      \"ref\": \"internal\",\n      \"modules\": {\n        \"2\": {\n          \"instrument_type\": \"QCM_RF\"\n        },\n        \"10\": {\n          \"instrument_type\": \"QRM_RF\"\n        }\n      }\n    }\n  },\n  \"hardware_options\": {\n    \"modulation_frequencies\": {\n      \"q00:mw-q00.01\": {\n        \"lo_freq\": 3946000000.0\n      },\n      ...\n    },\n    \"mixer_corrections\": {\n      \"q00:mw-q00.01\": {\n        \"dc_offset_i\": 0.0,\n        \"dc_offset_q\": 0.0,\n        \"amp_ratio\": 1.0,\n        \"phase_error\": 0.0\n      },\n      ...\n    }\n    },\n  \"connectivity\": {\n    \"graph\": {\n      \"directed\": false,\n      \"multigraph\": false,\n      \"graph\": {},\n      \"nodes\": [\n        {\n          \"instrument_name\": \"clusterA\",\n          \"id\": \"clusterA.module2.complex_output_0\"\n        },\n        ...\n      ],\n      \"links\": [\n        {\n          \"source\": \"clusterA.module2.complex_output_0\",\n          \"target\": \"q00:mw\"\n        },\n        ...\n      ]\n    }\n    }\n}\nThe file in the template package is cluster_configuration.json.\nYou can find more information about the hardware configuration in the documentation of quantify-scheduler\nMigrating old hardware configurations to match quantify-scheduler&gt;=0.18.0\nWith quantify-scheduler 0.18.0 there has been introduced a new way on how to structure the hardware configuration file. If you are having a hardware configuration file, that is structured using the old way, you can use the following script to migrate it to the new structure.\npython tergite_autocalibration/scripts/migrate_blox_hardware_configuration.py &lt;PATH_TO_HW_CONFIG&gt;\n\n\n\nWhile the previous two configuration files have been used to configure the room temperature instruments, the device configuration defines the initial parameters and characteristics of chip itself. The device configuration is having two main sections – the [device] and the [layout] section. In the [device] section prior knowledge about the device from the VNA are set for the resonator, qubit (drive) and the coupler.\nIt is possible to either address a qubit individually, e.g. the following would set the VNA frequency for qubit q06:\n[device.resonator.q06]\nVNA_frequency = 6832973301.189378\nor for all qubits:\n[device.resonator.all]\nattenuation = 12\n\n[device.qubit.all]\nmeasure.integration_time = 2.5e-6\nmeasure_1.integration_time = 2.5e-6\nmeasure_2.integration_time = 2.5e-6\nrxy.duration = 28e-9\nIn the [layout] section the positions of the qubits can be set. This is useful if one would like to e.g. plot the device. Qubits have an x (column) and a y (row) position:\n[layout.resonator.q06]\nposition = { column = 0, row = 0 }\n\n\n\nIn this file there are some settings such as the target node, the qubits and the couplers to calibrate.\nExample: Calibrate qubits q01 and q02 with coupler q01_q02 up until the node cz_calibration.\ntarget_node = \"cz_calibration\"\nqubits = [\"q01\", \"q02\"]\ncouplers = [\"q01_q02\"]\n\n\n\nBelow, you can define node-specific parameters setting [node_name.scope.property] where scope are the qubits/couplers and the property is a property known to the node. This would load and overwrite the configurations made in the device configuration.\nExample: Setting the reset duration for the resonator spectroscopy node.\n[resonator_spectroscopy.all]\nreset.duration =  60e-6\nThe file in the template package is node_config.toml.\n\n\n\nWhen working with two-qubit gates, there has to be a current source for the coupler and in the QBLOX stack this is coming from the so called SPI rack. The SPI configuration is mapping the qubits to their respective modules in the SPI rack and can be further used to assign the couplers to groups.\nExample: Definition of a coupler\n[couplers.q11_q12]\nspi_module_no = 1\ndac_name = \"dac0\"\nedge_group = 1\nThe file in the template package is spi_config.toml.\n\n\n\nIf you want to generate samplespaces with your own custom Python scripts, you can add a custom user samplespace configuration. The file must contain the definition of your samplespace according to the following schema:\nuser_samplespace = {\n    node1_name : {\n            \"settable_of_node1_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node1_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        },\n    node2_name : {\n            \"settable_of_node2_1\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            \"settable_of_node2_2\": { 'q01': np.ndarray, 'q02': np.ndarray },\n            ...\n        }\n}\nPlease note: Do not rename the variable user_samplespace, because it cannot be imported otherwise.\nThe file in the template package is user_samplespace.py.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/configuration_files.html#next-steps",
    "href": "user-guide/configuration_files.html#next-steps",
    "title": "Configuration",
    "section": "",
    "text": "Read about the commandline interface, which contains a chapter about how to load and save configuration packages.",
    "crumbs": [
      "Home",
      "User Guide",
      "Configuration Files"
    ]
  },
  {
    "objectID": "user-guide/operation.html",
    "href": "user-guide/operation.html",
    "title": "Operation",
    "section": "",
    "text": "The package ships with a CLI called acli (autocalibration command line interface) to solve some common tasks that appear quite often. In the following there are a number of useful commands, but if you want to find out information about commands in your shell use acli --help.\nSince some of the commands below are using autocompleting in most cases you would have to enable that feature in your shell by running:\nacli --install-completion\nUsually the shell would just suggest you file paths when pressing the tabulator, but with autocompletion enabled, you would be even get suggestions for node names or other inputs depending on your configuration.\n\n\nThis section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\nconfig: Load and save the configuration files\ncalibration: Handle operations related to the calibration supervisor\nbrowser: Will open the dataset browser, which makes you view the datasets from measurements\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nLoad the configuration.\nUsage:\nacli config load [OPTIONS]\nOptions:\n\n-f/--filepath: Path to the configuration package to load. It can be either to the configuration.meta.toml file or to a zip file containing the whole configuration.\n-t/--template: Path to the template package to load. The templates are located in tergite_autocalibration.config.templates. If the autocompletion is installed for acli, then the templates should be shown as suggestions automatically.\n\nNotes:\nTo run this command, please navigate to the root directory of the repository. The configuration package will be placed into the root directory, which is the default location for the application to detect the configuration package.\n\n\n\nUsage:\nacli config save [OPTIONS]\nSave the configuration.\nOptions:\n\n-f/--filepath: Path to the configuration package to save. If the path name is ending with .zip, it will automatically create a zip file and treat it as if you are running with -z.\n-z/--as-zip: Will make the configuration file be a zip archive.\n\n\n\n\nUsage:\nacli quickstart\nLoads the default configuration package. This configuration package is not supposed to run on any setup, but it will just copy configuration files into the application, so, that the application does not crash because configuration files are missing.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "user-guide/operation.html#cli",
    "href": "user-guide/operation.html#cli",
    "title": "Operation",
    "section": "",
    "text": "This section provides an overview of the Command Line Interface (CLI) options and their functionalities.\n\n\nThe autocalibration CLI is organized into several main command groups:\n\ncluster: Handle operations related to the cluster\nnode: Handle operations related to the node\ngraph: Handle operations related to the calibration graph\nconfig: Load and save the configuration files\ncalibration: Handle operations related to the calibration supervisor\nbrowser: Will open the dataset browser, which makes you view the datasets from measurements\njoke: Handle operations related to the well-being of the user\n\n\n\n\n\n\nReboots the cluster.\nUsage:\nacli cluster reboot\nThis command will prompt for confirmation before rebooting the cluster, as it can interrupt ongoing measurements.\n\n\n\n\n\n\nResets all parameters in Redis for the specified node(s).\nUsage:\nacli node reset [OPTIONS]\nOptions:\n\n-n, --name TEXT: Name of the node to be reset in Redis (e.g., resonator_spectroscopy)\n-a, --all: Reset all nodes\n-f, --from_node TEXT: Reset all nodes from the specified node in the chain\n\n\n\n\n\n\n\nPlots the calibration graph to the user-specified target node in topological order.\nUsage:\nacli graph plot\nThis command visualizes the calibration graph using an arrow chart.\n\n\n\n\n\n\nLoad the configuration.\nUsage:\nacli config load [OPTIONS]\nOptions:\n\n-f/--filepath: Path to the configuration package to load. It can be either to the configuration.meta.toml file or to a zip file containing the whole configuration.\n-t/--template: Path to the template package to load. The templates are located in tergite_autocalibration.config.templates. If the autocompletion is installed for acli, then the templates should be shown as suggestions automatically.\n\nNotes:\nTo run this command, please navigate to the root directory of the repository. The configuration package will be placed into the root directory, which is the default location for the application to detect the configuration package.\n\n\n\nUsage:\nacli config save [OPTIONS]\nSave the configuration.\nOptions:\n\n-f/--filepath: Path to the configuration package to save. If the path name is ending with .zip, it will automatically create a zip file and treat it as if you are running with -z.\n-z/--as-zip: Will make the configuration file be a zip archive.\n\n\n\n\nUsage:\nacli quickstart\nLoads the default configuration package. This configuration package is not supposed to run on any setup, but it will just copy configuration files into the application, so, that the application does not crash because configuration files are missing.\n\n\n\n\n\n\nStarts the calibration supervisor.\nUsage:\nacli calibration start [OPTIONS]\nOptions:\n\n-c TEXT: Cluster IP address (if not set, it will use CLUSTER_IP from the .env file)\n-r TEXT: Rerun an analysis (specify the path to the dataset folder)\n-n, --name TEXT: Specify the node type to rerun (works only with -r option)\n--push: Push a backend to an MSS specified in MSS_MACHINE_ROOT_URL in the .env file\n--browser: Will open the dataset browser in the background and plot the measurement results live\n\n\n\n\n\n\n\nStarts the dataset browser.\nUsage:\nacli browser --datadir [OPTIONS]\nOptions:\n\n--datadir PATH: Folder to take the plot data from\n--liveplotting: Whether plots should be updated in real time (default: False)\n--log-level INT: Log-level as in the Python logging package to be used in the logs (default: 30)\n\n\n\n\n\n\n\nPrints a random joke to lighten the mood.\nUsage:\nacli joke\nThis command fetches and displays a random joke, excluding potentially offensive content.\n\n\n\n\n\nThe CLI uses the Python library typer for command-line interface creation.\nSome commands may require additional configuration or environment variables to be set.\nWhen using the -r option for rerunning analysis, make sure to also specify the node name using -n.\n\nFor more detailed information about each command, use the --help option with any command or subcommand.",
    "crumbs": [
      "Home",
      "User Guide",
      "Operation"
    ]
  },
  {
    "objectID": "nodes/qubit_spectroscopy_node.html",
    "href": "nodes/qubit_spectroscopy_node.html",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "Qubit spectroscopy is a vital technique for identifying qubit resonance frequencies. By applying a probing signal to a qubit at various frequencies and measuring the response, we can accurately locate resonance frequencies and optimize qubit operation. In this node, both qubit frequecies for both 01 and 12 can be attained depending on the initial qubit state.\n\n\nThe Two_Tones_Multidim class facilitates the creation of schedules for qubit spectroscopy experiments. It supports multi-qubit spectroscopy, enabling parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nFrequency Sweeping: Iteratively adjusts the probing frequency and amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nspec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each qubit.\nspec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the probing pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe QubitSpectroscopyMultidim class analyzes the results of qubit spectroscopy experiments. The resonance peak is identified, enabling the determination of resonance frequencies.\n\n\nThe analyse_qubit method processes the spectroscopy data to extract key parameters:\n\nQubit Frequency: The frequency at which the qubit exhibits a resonance peak.\nOptimal Spectroscopy Amplitude: The amplitude yielding the strongest response.\n\nSteps:\n\nExtract the relevant coordinates (frequencies and amplitudes) from the dataset.\nIdentify the resonance peak.\nValidate the peak based on prominence and width criteria.\n\nReturns:\n\nA list containing the qubit frequency and the optimal spectroscopy amplitude.\n\n\n\n\nDetermines if a resonance peak exists in the data using statistical filters and peak detection.\nParameters:\n\nx (array): Data array to evaluate.\nprom_coef (float): Prominence coefficient.\nwid_coef (float): Width coefficient.\noutlier_median (float): Threshold for filtering outliers.\n\nReturns:\n\nA boolean indicating whether a peak is present."
  },
  {
    "objectID": "nodes/qubit_spectroscopy_node.html#qubit-spectroscopy-calibration-and-analysis",
    "href": "nodes/qubit_spectroscopy_node.html#qubit-spectroscopy-calibration-and-analysis",
    "title": "Tergite Automatic Calibration",
    "section": "",
    "text": "Qubit spectroscopy is a vital technique for identifying qubit resonance frequencies. By applying a probing signal to a qubit at various frequencies and measuring the response, we can accurately locate resonance frequencies and optimize qubit operation. In this node, both qubit frequecies for both 01 and 12 can be attained depending on the initial qubit state.\n\n\nThe Two_Tones_Multidim class facilitates the creation of schedules for qubit spectroscopy experiments. It supports multi-qubit spectroscopy, enabling parallel probing and measurement.\n\n\nThe schedule_function generates an experimental schedule for performing qubit spectroscopy. The sequence involves:\n\nReset: Resets all qubits to a known state.\nInitialize Qubit: Initialize the qubit to state 0 or 1, depending what qubit frequency you want to attain.\nFrequency Sweeping: Iteratively adjusts the probing frequency and amplitude.\nMeasurement: Captures the qubit response at each probing point.\n\nParameters:\n\nspec_frequencies (dict[str, np.ndarray]): Frequencies to probe for each qubit.\nspec_pulse_amplitudes (dict[str, np.ndarray], optional): Amplitudes of the probing pulses.\nrepetitions (int): Number of times the schedule will repeat.\nqubit_state (int): The state for the qubit.\n\nReturns:\n\nA Schedule object representing the experimental procedure.\n\n\n\n\n\nThe QubitSpectroscopyMultidim class analyzes the results of qubit spectroscopy experiments. The resonance peak is identified, enabling the determination of resonance frequencies.\n\n\nThe analyse_qubit method processes the spectroscopy data to extract key parameters:\n\nQubit Frequency: The frequency at which the qubit exhibits a resonance peak.\nOptimal Spectroscopy Amplitude: The amplitude yielding the strongest response.\n\nSteps:\n\nExtract the relevant coordinates (frequencies and amplitudes) from the dataset.\nIdentify the resonance peak.\nValidate the peak based on prominence and width criteria.\n\nReturns:\n\nA list containing the qubit frequency and the optimal spectroscopy amplitude.\n\n\n\n\nDetermines if a resonance peak exists in the data using statistical filters and peak detection.\nParameters:\n\nx (array): Data array to evaluate.\nprom_coef (float): Prominence coefficient.\nwid_coef (float): Width coefficient.\noutlier_median (float): Threshold for filtering outliers.\n\nReturns:\n\nA boolean indicating whether a peak is present."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This guide contains some information on how to get started with the automatic calibration. Please consider also reading the README.md file in the git repository for more information.\n\n\nThe automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.\n\n\n\nThe first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.10\nconda create -n tac python=3.10 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\nConfiguration files in case you are interested on how to configure the application and run the first experiments.\nDeveloper guide in case you would like to start developing features for the automatic calibration.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#prerequisites",
    "href": "getting_started.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "The automatic calibration requires redis for on-memory data storage. As redis operates only on Linux systems, the calibration has to run either on one of:\n\nLinux distributions\nWSL (Windows Subsystem for Linux) environments, installed on a Windows system. WSL requires Windows 10 and a version of at least 1903.\n\nInstallation instructions for redis can be found here: https://redis.io/docs/getting-started/installation/install-redis-on-linux/\nThe link for the redis installation also contains instructions on how to start a redis instance. However, if you already have redis installed, you can run it using:\nredis-server --port YOUR_REDIS_PORT\nUsually, redis will run automatically on port 6379, but in a shared environment please check with others to get your redis port, since you would overwrite each other’s memory.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "getting_started.html#installation",
    "href": "getting_started.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "The first step during the installation is to clone the repository. Please note that the link below is the link to the public mirror of the repository on GitHub. If you are developing code, most likely, you have to replace it with the link to the development server.\ngit clone git@github.com:tergite/tergite-autocalibration.git\nTo manage Python packages, we are using the package manager conda. It is recommended to create an environment for your project - alternatively, you can also just use Python 3.10\nconda create -n tac python=3.10 -y\nHere, tac stands for tergite-autocalibration. We can activate and use the environment like this:\nconda activate tac\nIf you are not using conda, activate the environment with:\nsource activate tac\nNow, you should enter the repository folder, because the following commands have to be executed in there.\ncd tergite-autocalibration\nTo install the repository in editable mode. In Python the editable mode is triggerd by the parameter -e. This means that when you changed and saved your code files, they will be automatically loaded into the environment without re-installation.\npip install -e .\nHere . is the root directory (i.e. the directory that contains the pyproject.toml file)\n\n\n\nConfiguration files in case you are interested on how to configure the application and run the first experiments.\nDeveloper guide in case you would like to start developing features for the automatic calibration.",
    "crumbs": [
      "Home",
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "developer_guide.html",
    "href": "developer_guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "This and the following sections provide information on how to develop code in tergite-autocalibration.\n\n\nConsider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests\n\n\n\nWe use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case\n\n\n\n\nThere are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.\n\n\n\n\nPlease read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML\n\n\n\n\nAs you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#additional-installations",
    "href": "developer_guide.html#additional-installations",
    "title": "Developer Guide",
    "section": "",
    "text": "Consider installing Quarto and other packages before you create your coda environment to have the path correctly initialised in your environment. This is not necessary, but it can simplify operations later, especially using VSCode.\nAfter you install tergite-autocalibration with:\npip install -e .\nrun\npip install poetry\npoetry install --with dev,test\nthis will install the additional packages for developing code and running tests",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#naming-convention-and-style",
    "href": "developer_guide.html#naming-convention-and-style",
    "title": "Developer Guide",
    "section": "",
    "text": "We use American English, please set any spell checker to this language.\n\nThe file names should be written using snake_case, with words written in lowercase and separated by underscores.\nClass names should be in PascalCase (where all words are capitalized). Many class do not follow this rule and use CamelCase with underscore, they will be changed.\nMethods should be in snake_case\nVariables should be in snake_case",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#ide-preloaded-settings",
    "href": "developer_guide.html#ide-preloaded-settings",
    "title": "Developer Guide",
    "section": "",
    "text": "There are settings available in the repo for IDEs, recommending extensions and settings. Please discuss with the team before modifying these default settings, they should be changed only with the consensus of the team.\n\n\nBlack, quarto and python extensions are recommended. Some settings are recommended too. You can find the settings for VSCode in the repository in the folder .vscode.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#things-to-do-before-a-commit",
    "href": "developer_guide.html#things-to-do-before-a-commit",
    "title": "Developer Guide",
    "section": "",
    "text": "Please read carefully below, what should be done before doing a commit to a merge request. Most of the points are going to be checked automatically in GitLab, so, you would receive an error when running the pipeline.\n\n\nWhen submitting contributions, please prepend your commit messages with:\n\nfix: for bug fixes\nfeat: for introducing and working on a new feature (e.g. a new measurement node or a new analysis class)\nchore: for refactoring changes or any change that doesn’t affect the functionality of the code\ndocs: for changes in the README, docstrings etc\ntest: or dev: for testing or development changes (e.g. profiling scripts)\n\n\n\n\nWhen you create or modify a file, make sure that add the following copyright text is present at the top of the file. Remember to add your name and do not delete previous contributors. If you add the statement to a file that does not have one, please check on git the names of contributors.\n# This code is part of Tergite Autocalibration\n#\n# (C) Copyright WRITE YOUR NAME HERE, 2024\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\n\nUpdate the changelog in the section called [Unreleased]. Please note that there are several sections titled “Added”, ” Change” and “Fixed”; place your text in the correct category.\n\n\n\nThe code analyzer used in the project is black, which is installed as part of the dev dependencies. To use black, open a shell and run\nblack .\nPlease make sure to run it before committing to a merge request.\nIf your pipeline is still showing an error when you are running black, please make sure that you have installed the right version of black. You can check that by running\nblack --version\nIf your black version differs, it might be possible that you have had installed a version of black either in your base environment or via apt. To remove these versions, please deactivate your conda environment with conda deactivate and then run:\nsudo apt remote black\nor\npip uninstall black\nNext, please activate again your Python environment and install the correct version black as defined in the pyproject.toml file.\npip install black==VERSION_FROM_PYPROJECT_TOML",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer_guide.html#next-steps",
    "href": "developer_guide.html#next-steps",
    "title": "Developer Guide",
    "section": "",
    "text": "As you noticed, most of the above advice contains the formatting and other formal steps during development. Consider reading about:\n\nUnit testing to find out more about how to write test cases for the code.\nNodes on how to create a new calibration node.\nNode Classes to learn about the different types of nodes that the framework supports.\n\nIf you are having any feedback about the documentation or want to work on documentation, please continue with this developer guide about how to contribute with documentation.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Introduction"
    ]
  },
  {
    "objectID": "developer-guide/node_classes.html",
    "href": "developer-guide/node_classes.html",
    "title": "Node classes",
    "section": "",
    "text": "Node classes\nThe execution of most of the nodes consists of a single schedule compilation, a single measurement and a single post-processing. Although for most of the nodes this workflow suffices, there are exceptions while this workflow can become limiting in more advanced implementations.\nTo allow greater flexibility in the node implementations the nodes are categorized:\n\nScheduleNode: The simple way of doing the measurement and having an analysis afterward. This node compiles one time\n\nThere is only node.schedule_samplespace if the sweeping takes place within the schedule.\n\nExternalParameterNode: A looping over an external parameter during the sweep. This node compiles several times.\n\nThere are both node.schedule_samplespace and node.external_samplespace if there are sweeping parameters outside the schedule. For example the coupler_spectroscopy node sweeps the dc_current outside of the schedule:\n\n\nBelow, there is an example for an ExternalParameterNode implementation.\nimport numpy as np\n\nfrom tergite_autocalibration.lib.nodes.coupler.spectroscopy.analysis import (\n    CouplerSpectroscopyNodeAnalysis,\n)\n\nfrom tergite_autocalibration.lib.nodes.external_parameter_node import (\n    ExternalParameterNode,\n)\n\nfrom tergite_autocalibration.lib.nodes.qubit_control.spectroscopy.measurement import (\n    Two_Tones_Multidim,\n)\n\nfrom tergite_autocalibration.lib.utils.samplespace import qubit_samples\nfrom tergite_autocalibration.utils.dto.enums import MeasurementMode\nfrom tergite_autocalibration.utils.hardware.spi import SpiDAC\n\n\nclass CouplerSpectroscopyNode(ExternalParameterNode):\n    measurement_obj = Two_Tones_Multidim\n    analysis_obj = CouplerSpectroscopyNodeAnalysis\n    coupler_qois = [\"parking_current\", \"current_range\"]\n\n    def __init__(\n            self, name: str, all_qubits: list[str], couplers: list[str], **schedule_keywords\n    ):\n        super().__init__(name, all_qubits, **schedule_keywords)\n        self.name = name\n        self.couplers = couplers\n        self.qubit_state = 0\n        self.schedule_keywords[\"qubit_state\"] = self.qubit_state\n        self.coupled_qubits = self.get_coupled_qubits()\n        self.coupler = self.couplers[0]\n\n        self.mode = MeasurementMode.real\n        self.spi_dac = SpiDAC(self.mode)\n        self.dac = self.spi_dac.create_spi_dac(self.coupler)\n\n        self.all_qubits = self.coupled_qubits\n\n        self.schedule_samplespace = {\n            \"spec_frequencies\": {\n                qubit: qubit_samples(qubit) for qubit in self.all_qubits\n            }\n        }\n\n        self.external_samplespace = {\n            \"dc_currents\": {self.coupler: np.arange(-2.5e-3, 2.5e-4, 280e-6)},\n        }\n\n    def get_coupled_qubits(self) -&gt; list:\n        if len(self.couplers) &gt; 1:\n            print(\"Multiple couplers, lets work with only one\")\n        coupled_qubits = self.couplers[0].split(sep=\"_\")\n        self.coupler = self.couplers[0]\n        return coupled_qubits\n\n    def pre_measurement_operation(self, reduced_ext_space):\n        iteration_dict = reduced_ext_space[\"dc_currents\"]\n        this_iteration_value = list(iteration_dict.values())[0]\n        print(f\"{ this_iteration_value = }\")\n        self.spi_dac.set_dac_current(self.dac, this_iteration_value)\n\n    def final_operation(self):\n        print(\"Final Operation\")\n        self.spi_dac.set_dac_current(self.dac, 0)\nPlease read the guide about how to create a new node to learn more about nodes. This guide also contains an example for a ScheduleNode.\nExamples of nodes requiring an external samplespace\n\ncoupler_spectroscopy sweeps the dc_current which is set by the SPI rack not the cluster\nT1 sweeps a repetition index to repeat the measurement many times\nrandomized_benchmarking sweeps different seeds. Although the seed is a schedule parameter, sweeping outside the schedule improves memory utilization.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Node Classes"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html",
    "href": "developer-guide/unit_tests.html",
    "title": "Unit tests",
    "section": "",
    "text": "When testing software, there are several ways to test whether it is working. Amongst others such as system integration tests or usability scores, there are unit tests. A unit test is meant to confirm whether a small unit of the code is working such as a function or a class. The idea behind unit tests though, is to create tiny little tests for each function, that checks how it handles:\n\nNormal case: The things that you would expect a function to do e.g. if you have addition, and you add natural numbers.\nEdge cases: In the example with the addition this would be e.g. whether it correctly adds zero or would subtract if there is a negative number.\nFail cases: Let us say you have addition, and you are adding a number such as 20 with the string “hello”. This should fail.\n\nHaving thought about the test cases and possible scenarios will help to get a better understanding what the code does, also, it will make the code more robust. In our code base we are having a pipeline that will automatically run all tests as soon as someone wants to merge to the common branches. There are two locations where tests are stored:\n\nIn the folder for tests: This is a folder on the main level of the repository where more general tests for the whole framework go.\nIn the folder of each node: This is to test only the node itself. The tests are added directly to the node module.\n\nSince it happens more often that one will write tests for the node itself, in the following, there will be a section explaining how to do it on an example node.\n\n\nThese instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\ndef test_canCreateCorrectType():\n  c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\ndef test_CanGetQubitsFromCouplers():\n  c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n      CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n            Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.\n\n\n\n\nWhen running unit tests, the test framework itself and the way tests are executed can cause problems that make the unit tests fail. Hence, it is important to understand how pytest works to get a grip on how to debug failing tests efficiently.\nIn the beginning of each test run, pytest will try to import all necessary modules. If there are any “Unresolved reference” errors, the tests will even not start to run.\n\n\nTests might even indirectly affect the outcome of another test. As an example, there might be a test, which changes some global state in the application. Let us say the default value for the global variable VAR_1 is 0, but one of the test changes sets the global variable VAR_1 to 1. Now, all following tests will read 1 when they get VAR_1 from the environment.\nSometimes though, it might be necessary to change to global variable of one test. To handle these situations, the test framework has implemented some decorators, which intend to freeze the environment variables. The most simple way to freeze the state of the environment and then e.g. set variables inside the test function is with the @preserve_os_env decorator.\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import preserve_os_env\n\n\n@preserve_os_env\ndef test_my_function_that_sets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  os.environ[\"VAR_1\"] = \"1\"\n  new_var1 = os.environ[\"VAR_1\"]\n  assert int(var1) + 1 == int(new_var1)\n\n\ndef test_my_function_that_only_gets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  assert int(var1) == 0\nHere, if we had run the first test without the @preserve_os_env decorator, the second test would fail, because the variable would be set to 1 in the first test and does not have the default value 0.\nAnother decorator in that regard is the @with_os_env decorator, which takes as an input the dictionary of values that environment should hold. It is just a way that - similarly to a fixture - simplifies the way things are set up without having too many os.environ calls inside the code. For example, you want to change the value of an environmental variable in exactly one test function, you can add:\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import with_os_env\n\n\n@with_os_env({\"VAR_1\": \"1\"})\ndef test_my_function_that_needs_the_os_variables_changed():\n  var1 = os.environ[\"VAR_1\"]\n  assert var1 == \"ABC\"\nThis will freeze the current state of the environment, replace the variable VAR_1 with “ABC” and after the test will cleanup the environment and load the previous values.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "href": "developer-guide/unit_tests.html#unit-tests-for-a-node",
    "title": "Unit tests",
    "section": "",
    "text": "These instructions will go step-by-step through how to create meaningful test cases for a node.\n\nOverview about the folder structure\nSpecific advices on how to test nodes\nExamples on how to test the analysis function of a node\n\nIf you are more a person that learns from the code rather than from a tutorial, please take a look at an easy node e.g. resonator spectroscopy and try to run and understand the test cases.\n\n\nThe test should be created in a sub-folder of the node called tests.\nPlease organise the file using these sub-folders:\n\ndata: place here any data file that is needed to create test cases, while it is possible to mock the data. Feel free to add a text file explaining how the data was produced.Mocking data is also possible, but it may be not practical for complex datasets.\nresults: create this folder to store your results, i.e. files that would be created by your analysis such as the plots. It should be empty, do not commit your results. To assure this is the case, add a file name .gitignore in the folder with this code:\n\n# Ignore everything in this directory\n*\n\n# But do not ignore this file\n!.gitignore     \n\n\n\nA good starting point, especially starting from scratch, it is to test for the class type, then that the inputs have the right formats and then move to some simple operation or trivial case. Build more complex cases from the simpler ones and exploits your tests to refactor the code as needed. Try to test as many reasonable cases as possible, both successfully and not. Remember to test for exceptions. We also suggest to develop using test drive development techniques that will ensure a high test coverage and a good code structure. Yes, do not forget to keep refactoring to improve the code.\nYou can find some samples below taken from the cz_parametrisation node, where all objects are tested\nCurrently, there is no way to differentiate from tests that require a QPU (i.e. measurements) and those that do not ( i.e. analyses). Since the latter are simpler to write, start with those that, in general, are more likely to benefit from unit tests as there is much more logic in the analysis than in the measurement.\nIf you need to test some complex scenario, such as those involving sweep, it is probably easier to start writing code and tests from the lowest level and then compose those objects to handle the more complex scenario considered.\n\n\n\nTest class type\ndef test_canCreateCorrectType():\n  c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert isinstance(c, CZ_Parametrization_Fix_Duration_Node)\n    assert isinstance(c, ScheduleNode)\nThe suggested very first test is to instantiate the class and make sure it has the correct type(s) following any inheritance.\nTest input parameters\ndef test_CanGetQubitsFromCouplers():\n  c = CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q15\"])\n    assert c.all_qubits == [\"q14\", \"q15\"]\n    assert c.couplers == ['q14_q15']\nMake sure all inputs and their manipulations are correctly initialised in the constructor, in this case the qubits are taken from the coupler pair\nTest exception\ndef test_ValidationReturnErrorWithSameQubitCoupler():\n    with pytest.raises(ValueError):\n      CZ_Parametrization_Fix_Duration_Node(\"cz_char_fixCurrent\", couplers=[\"q14_q14\"])\nInputs can be incorrect and should always be tested to avoid unexpected behaviour down the line which can be difficult to trace back to the origin. There are infinite number of possible errors, so it is impossible to cover them all, but at least the obvious ones should be considered. In this case a typical typing error with the couple have the same qubit twice.\nTest with data\n@pytest.fixture(autouse=True)\ndef setup_good_data():\n    os.environ[\"DATA_DIR\"] = str(Path(__file__).parent / \"results\")\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    return d14, d15, freqs, amps\nThis is an example of how to load data for testing the analysis of a node, please note that the specifics may change as we are about to change the dataset format at the time of writing this; however, the principle will be the same.\nThis data can then be used in multiple tests, for example:\ndef test_canGetMaxFromQ1(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 9\n    assert indexBestAmp[0] == 13\n\n\ndef test_canGetMinFromQ2(setup_good_data):\n    d14, d15, freqs, amps = setup_good_data\n    c = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c.run_fitting()\n    indexBestFreq = np.where(freqs == result[0])[0]\n    indexBestAmp = np.where(amps == result[1])[0]\n    assert indexBestFreq[0] == 10\n    assert indexBestAmp[0] == 12\nThese two tests make sure that the return values are correct for the two qubits that are connected by the coupler.\nTest creating of images from plotter\ndef test_canPlotBad(setup_bad_data):\n    matplotlib.use(\"Agg\")\n    d14, d15, freqs, amps = setup_bad_data\n    c14 = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    result = c14.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q14.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c14.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\n\n    c15 = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    result = c15.run_fitting()\n\n    figure_path = os.environ[\"DATA_DIR\"] + \"/Frequency_Amplitude_bad_q15.png\"\n    # Remove the file if it already exists\n    if os.path.exists(figure_path):\n        os.remove(figure_path)\n\n    fig, ax = plt.subplots(figsize=(15, 7), num=1)\n    plt.Axes\n    c15.plotter(ax)\n    fig.savefig(figure_path)\n    plt.close()\n\n    assert os.path.exists(figure_path)\n    from PIL import Image\n\n    with Image.open(figure_path) as img:\n        assert img.format == \"PNG\", \"File should be a PNG image\"\nThis is an example on how to save files in the “results” sub-folder within the “tests” folder. The code make sure the expected file exists and has the correct format. The created files can be inspected by the developer. A possible extension would be to upload the images in the data folder and check the those produced by the test are identical.\nComplex dataset for comparing results\n@pytest.fixture(autouse=True)\ndef setup_bad_data():\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    return q14Res, q15Res\n\n\ndef test_combineBadResultsReturnNoValidPoint(setup_bad_data):\n    q14Res, q15Res = setup_bad_data\n    c = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n    r = c.are_frequencies_compatible()\n    assert r == False\n    r = c.are_amplitudes_compatible()\n    assert r == False\n    r = c.are_two_qubits_compatible()\n    assert r == False\nIn this example, the data produced is loaded from a file that has data that is not a good working point. Note that there two analyses are run in the setup; the combination is run in the test, so that, if needed in other tests, the data could be modified for specific cases. This is a failure test, making sure that bad inputs are not recognised as good working points.\nEven more complex setup\ndef setup_data():\n    # It should be a single dataset, but we do not have one yet, so we loop over existing files\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs, amps)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs, amps)\n    q15Res = q15Ana.run_fitting()\n    c1 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = Path(__file__).parent / \"data\" / \"dataset_bad_quality_freq_amp.hdf5\"\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_bad = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_bad = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(\n        d14, freqs_bad, amps_bad\n    )\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(\n        d15, freqs_bad, amps_bad\n    )\n    q15Res = q15Ana.run_fitting()\n    c2 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    dataset_path = (\n            Path(__file__).parent / \"data\" / \"dataset_good_quality_freq_amp_2.hdf5\"\n    )\n    print(dataset_path)\n    ds = xr.open_dataset(dataset_path)\n    ds = ds.isel(ReIm=0) + 1j * ds.isel(ReIm=1)\n    d14 = ds.yq14.to_dataset()\n    d15 = ds.yq15.to_dataset()\n    d14.yq14.attrs[\"qubit\"] = \"q14\"\n    d15.yq15.attrs[\"qubit\"] = \"q15\"\n    freqs_2 = ds[f\"cz_pulse_frequenciesq14_q15\"].values  # MHz\n    amps_2 = ds[f\"cz_pulse_amplitudesq14_q15\"].values  # uA\n    q14Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q1_Analysis(d14, freqs_2, amps_2)\n    q14Res = q14Ana.run_fitting()\n    q15Ana = CZ_Parametrisation_Frequency_vs_Amplitude_Q2_Analysis(d15, freqs_2, amps_2)\n    q15Res = q15Ana.run_fitting()\n    c3 = CZ_Parametrisation_Combined_Frequency_vs_Amplitude_Analysis(q14Res, q15Res)\n\n    list_of_results = [(c1, 0.1), (c2, 0.2), (c3, 0.3)]\n    return list_of_results, freqs, amps, freqs_2, amps_2\nIn this case, three points are loaded and the analysis is run on each of them. The results are returned for each point and can be used in different tests, for example by removing elements in the list_results array.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  },
  {
    "objectID": "developer-guide/unit_tests.html#advanced-topics-on-unit-tests",
    "href": "developer-guide/unit_tests.html#advanced-topics-on-unit-tests",
    "title": "Unit tests",
    "section": "",
    "text": "When running unit tests, the test framework itself and the way tests are executed can cause problems that make the unit tests fail. Hence, it is important to understand how pytest works to get a grip on how to debug failing tests efficiently.\nIn the beginning of each test run, pytest will try to import all necessary modules. If there are any “Unresolved reference” errors, the tests will even not start to run.\n\n\nTests might even indirectly affect the outcome of another test. As an example, there might be a test, which changes some global state in the application. Let us say the default value for the global variable VAR_1 is 0, but one of the test changes sets the global variable VAR_1 to 1. Now, all following tests will read 1 when they get VAR_1 from the environment.\nSometimes though, it might be necessary to change to global variable of one test. To handle these situations, the test framework has implemented some decorators, which intend to freeze the environment variables. The most simple way to freeze the state of the environment and then e.g. set variables inside the test function is with the @preserve_os_env decorator.\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import preserve_os_env\n\n\n@preserve_os_env\ndef test_my_function_that_sets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  os.environ[\"VAR_1\"] = \"1\"\n  new_var1 = os.environ[\"VAR_1\"]\n  assert int(var1) + 1 == int(new_var1)\n\n\ndef test_my_function_that_only_gets_os_variables():\n  var1 = os.environ[\"VAR_1\"]  # VAR_1 = \"0\"\n  assert int(var1) == 0\nHere, if we had run the first test without the @preserve_os_env decorator, the second test would fail, because the variable would be set to 1 in the first test and does not have the default value 0.\nAnother decorator in that regard is the @with_os_env decorator, which takes as an input the dictionary of values that environment should hold. It is just a way that - similarly to a fixture - simplifies the way things are set up without having too many os.environ calls inside the code. For example, you want to change the value of an environmental variable in exactly one test function, you can add:\nimport os\nfrom tergite_autocalibration.tests.utils.decorators import with_os_env\n\n\n@with_os_env({\"VAR_1\": \"1\"})\ndef test_my_function_that_needs_the_os_variables_changed():\n  var1 = os.environ[\"VAR_1\"]\n  assert var1 == \"ABC\"\nThis will freeze the current state of the environment, replace the variable VAR_1 with “ABC” and after the test will cleanup the environment and load the previous values.",
    "crumbs": [
      "Home",
      "Developer Guide",
      "Unit tests"
    ]
  }
]